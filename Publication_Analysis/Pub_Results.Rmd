---
csl: inter-research-science-center.csl
header-includes: 
- \usepackage{setspace}
- \doublespacing
- \usepackage{rotating}
- \usepackage{booktabs}
- \usepackage{makecell}
- \usepackage{gensymb}
- \usepackage{upgreek}
- \usepackage{pdfpages}
- \usepackage{caption}
- \usepackage{float}
output: 
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
    keep_tex: yes
indent: true
---
```{r setup, include=FALSE}

library(knitr)
# Troubleshooting knitting to pdf
#options(tinytex.verbose = TRUE)

# set default settings to save figures in Figures folder
knitr::opts_knit$set(root.dir = normalizePath('../'), # set working directory to project level
                     fig.path = "./Pub_Manuscript/Figures")

# Set chunk options- no warning messages, and place figures near where they are referenced.             
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.width = 6,
                      fig.asp=0.618, 
                      fig.align = "center",
                      fig.pos='H')

```

```{r, message=TRUE, warning=TRUE, include=FALSE}
library(groundhog)
pkgs <- c("dplyr",
          "tidyr",
          "readr",
          "magrittr",
          "data.table",
          "lubridate",
          "ggplot2",
          "ggthemes",
          "stringr",
          "hms",
          "forcats",
          "gridExtra",
          "ggpmisc",
          "viridis",
          "scales",
          "cowplot",
          "wesanderson",
          "knitr",
          "tinytex",
          "broom",
          "agricolae",
          "multcompView",
          "quantreg",
          "grid",
          "pwr",
          "sqldf",
          "ggmosaic"
          )
groundhog.library(pkgs, "2020-03-01")
```


```{r load_packages, include=FALSE, eval=FALSE}
################### Packages ###########################
library(dplyr)
library(tidyr)
library(readr) # reading in files 
library(magrittr) # pipe function %>%
library(data.table) # file read function and needed for xtable
library(lubridate) # dealing with time and dates
library(ggplot2) # graphing
library(ggthemes) # theme_classic() for graphs
library(stringr) # dealing with text strings
library(hms) # dealing with time
library(forcats) # dealing with factors
library(gridExtra)
library(ggpmisc) # package for adding line equation and R^2 to plots
library(viridis) # Color palette - Options: magma , inferno, plasma, viridis, cividis
library(scales) # add different number scales in graphs
library(cowplot) # for arranging multiple plots into a single figure
library(wesanderson) # color palette
#library(olsrr) # regression output table
#library(MASS) # collection of datasets and functions
library(knitr)
library(tinytex) # LaTeX conversion package
library(broom) # tidy() function to convert t.test to dataframe
library(agricolae) # post-hoc Tukey test
library(multcompView) #
library(quantreg) # Quantile regression
library(grid) # makes test grobs useful for multiframe figures
library(pwr) # Power analysis
library(sqldf) # kevin particle analysis
library(ggmosaic) # mosaic plots
```

```{r create_directories, include=FALSE}
# source functions 
source("./functions.R")
# define output directory
output_directory = "./output"
# create output directories (same as Insitu_Filtraiton_Analysis_script.Rmd)
createOutputDirectories()
```

```{r read_in_data, include=FALSE}
##################### Read in Data from  project output ##########################
All_data <- fread(file.path('./output/4_Filtration_Calculations/Filtration_Summary', "Master_Filtration_analysis_table.csv"))
Avg_TPM_summary_table <- fread(file.path('./output/5_TPM_OC_Summary', "Avg_TPM_summary_table.csv"))
Density_directory = "./Data/bivalve_density_community"
NPD_bivalves = fread(file.path(Density_directory, "Insitu_Filter_NPD_bivalve_biomass_data.csv"))
Sbs_correction_data = fread(file.path('./output/6_Sbs_Corrections_Summary', "sbs_correction_summary.csv"))
```

```{r setup_data_wrangling, include=FALSE}
##################### Data #########################################
# Remove San Diego filtration trial - not applicable to this analysis - different experimental set up
Master_analysis_table <- All_data %>% 
  filter(!Site %in% c("San Diego"))

# Order Sites by Latitude
site_by_lat <- c("San Rafael", "Morro Bay", "Shellmaker", "Deanza") 
site_by_lat_abv <- c("SR", "MB", "NPSM", "NPD")

# transform Site variable to factor and arange by Latitude
Master_analysis_table <- arrange(transform(Master_analysis_table, Site = factor(Site,levels = site_by_lat)), Site)

# All Trials, NPD 2019_4_17 removed - below Chl detection limit
site_data_outrmd <- Master_analysis_table %>% 
  filter(round(Chl_ug_L_up,2) > 0.1 | round(Chl_ug_L_up,2) < -0.1)

 # Filtration trials only - exclude Negative controls
Filtration_only_data <- Master_analysis_table %>% 
  filter(Experiment %in% c("Filtration"))

# Filtration only - remove trials with Chl a within sensor detection limit (+- 0.1ug/L)
Filter_only_outRmd_data <- Filtration_only_data %>% 
  filter(round(Chl_ug_L_up,2) > 0.1 | round(Chl_ug_L_up,2) < -0.1)
```

```{r values_for_text, include=FALSE}
# SR filtration trial data
SR_filter_sum <- Filter_only_outRmd_data %>% 
  filter(Site %in% c("San Rafael"),
         Experiment %in% c("Filtration")) 

MB_filter_sum <- Filter_only_outRmd_data %>% 
  filter(Site %in% c("Morro Bay"),
         Experiment %in% c("Filtration"))

NPD_filter_sum <- Filter_only_outRmd_data %>% 
  filter(Site %in% c("Deanza"),
         Experiment %in% c("Filtration"))

NPSM_filter_sum <- Filter_only_outRmd_data %>% 
  filter(Site %in% c("Shellmaker"),
         Experiment %in% c("Filtration"))

# Single out NPD 2019_4_17 for stats reporting

NPD_2019_4_17 <- read_csv("output/4_Filtration_Calculations/Insitu_Filter_NPD_2019_4_17.csv")
NPD_2019_4_17 %<>% filter(Experiment == "Filtration") 

# Deanza only - Filtration trials only
Deanza_Filter_data <- Master_analysis_table %>% 
  filter(Site %in% c("Deanza"),
         Experiment %in% c("Filtration"))
# Deanza only- Positive Filtration only
Deanza_PosFilter_data <- Deanza_Filter_data %>% 
  filter(pcnt_Chl_rmvd > 0)

### Average TPM Data ######################
Avg_TPM_summary_table <- arrange(transform(Avg_TPM_summary_table, Site = factor(Site, levels = site_by_lat)), Site)

### Reporting Restoration vs aquaculture in Abstract
abstract_num <- Filter_only_outRmd_data %>% 
  mutate(habitat = ifelse(Site %in% c("San Rafael", "Deanza", "Shellmaker"), "Rest", "Aqcltr")) %>%
  dplyr::select(Site, L_Hr_m2, habitat) %>% 
  group_by(habitat) %>% 
  summarise(mean_HCR = mean(L_Hr_m2),
            SD = sd(L_Hr_m2),
            st_error = SD / sqrt(length(L_Hr_m2)))
```

```{r functions, include=FALSE}
###################### Functions ####################################

# function to calculate average, standard deviation, outlier data. 
find_outlier <- function(data){
  average <- mean(data)
  stdev <- sd(data)
  # Outlier data defined by outside 2.2 standard deviations from the mean 
  outlier <- ifelse((data < (average - 2.2 * stdev) | (data > (average + 2.2 * stdev))),
         TRUE, FALSE)
}
```

```{r figure_setup, include=FALSE}
############### Site Color Assignments ###############
viridis(4)
Site_colors <- c("San Rafael" = "#440154FF",
                 "Morro Bay" = "#31688EFF",
                 "Shellmaker" = "#35B779FF",
                 "Deanza" = "#FDE725FF")

############### Bivalve Species Color Assignments ###############
# wes_palettes # lists all current palattes 
# Color palette for Species - Royal2, Darjeeling2, Cavalcanti1, BottleRocket2, IsleofDogs1
density_palette <- rev(wes_palette("IsleofDogs1", 7, type = c("continuous")))
Species_colors <- c("Adula diegensis" = "#8D8680",
                    "Argopecten ventricosa" = "#CCC3C5",
                    "Crassostrea gigas" = "#6D6340",
                    "Geukensia demissa" = "#524D4F",
                    "Musculista senhousia" = "#B0915B",
                    "Mytilus galloprovincialis" = "#7E4B41",
                    "Ostrea lurida" = "#9986A5",
                    "Unknown mussel" = "black")

################# Labels for graphs  #########################
Hab_FR_Label <- expression('Habitat Clearance Rate (L hr'^-1*'m'^-2*')')
Prt_Chl_Label <- expression(paste('Percent Chlorophyll ', alpha, ' Removed'))
Chl_ugL_Label <- expression(paste("Chlorophyll ", alpha, " (", mu, "g/L) "))
TPM_Label <- c('Total Particulate Matter mg/L')
PIM_Label <- c('Particulate Inorganic Matter mg/L')


```

# Results

Twenty-one filtration trials, across the four study sites, were included in the analyses. 
A single filtration trial at Deanza (2019-4-17; Table 4) was removed from the analysis because the mean upstream Chl $\alpha$ (*M* = `r round(mean(NPD_2019_4_17$Chl_ug_L_up),1)`, *SD* = `r round(sd(NPD_2019_4_17$Chl_ug_L_up),2)`) was within the detection limit of the sensor (± 0.1 μg/L). 
Filtration trials across sites were not distributed equally (Table 4), Deanza had more than twice the amount filtration trials (*N* = 9) as San Rafael (*N* = 4), Morro Bay (*N* = 4), and Shellmaker (*N* = 4).

## Habitat Clearance Rates 

```{r Random_Forest_DrNichols, include=FALSE}
###### Estimate missing values (Imputation) 
#In our data we are missing 4 out of 400ish values.  (cells)
#Because we have limited observations but a high number of variables simply
#removing incomplete observations is too costly

#For imputation we are doing a non parametric random forest imputation.  See missForest documentation for details/references.  
#Stekhoven, D.J. and Buehlmann, P. (2012), "missForest - nonparametric missing value imputation for mixed-type data', Bioinformatics, 28(1) 2012, 112-118, doi: 10.1093/bioinformatics/btr597


# Import data
Analysis_data <- Master_analysis_table %>% 
    mutate(Date = mdy(Date),
         L_hr_m2 = L_Hr_m2,
         pcnt_Chl_rmvd = pcnt_Chl_rmvd,
         Temp_C_Up = Temp_C_Up,
         Sal_ppt_Up = Sal_ppt_Up,
         Turbidity_NTU_Up = Turbidity_NTU_Up,
         Avg_TPM_mg_L = Avg_TPM_mg_L,
         Avg_OC_Ratio = Avg_OC_Ratio,
         Chl_ug_L_Up = Chl_ug_L_up,
         Chl_ug_L_Down = Chl_ug_L_down,
         avg_depth_m = avg_depth_m,
         d_bw_sondes_m = d_bw_sondes_m,
         avg_m_sec = avg_m_hr/3600) %>%
  dplyr::select(Site, Date, Experiment, Temp_C_Up, Sal_ppt_Up, Turbidity_NTU_Up, Avg_TPM_mg_L, 
          Avg_OC_Ratio, Chl_ug_L_Up, Chl_ug_L_Down, avg_depth_m, d_bw_sondes_m, avg_m_sec, 
          pcnt_Chl_rmvd, L_hr_m2) %>% 
 dplyr::arrange(Site, Date)
  
# packages needed for imputation and randomforest regression
library(missForest)
library(rpart)
library(randomForest)
library(adabag)

# Imputation for 4 missing values (TPM & OC)
temp1 = Analysis_data[,c(1,2,3)] # temporarily remove data labels for imputation
temp2 = missForest(Analysis_data[,-c(1,2,3)])$ximp # Imputation of missing values
test = cbind(temp1,temp2) # bring back data labels 
# dimensions of data with labels removed (data only) 28 x 12 = 336 data cells
dim(Analysis_data[,-c(1,2,3)]) 
num_NA_values <- sum(is.na(Analysis_data)) # 4 missing values

colnames(test) <- c( "Site", "Date", "Experiment", "Temp","Salinity", "Turbidity",
                            "TPM", "OC", "Chl.up", "Chl.dn", 
                           "Depth","Distance", "Water velocity", "Chl.removal", 
                           "Clearance")

#Shellmaker 06-09 Neg Control, Shellmaker 05-22 Filtration and Deanza 4-17 Filtration just have astronomical outlier values for clearance.
#Lets ignore them for now as they complete dominate any model useful for comparison.

##### Habitat Clearance Rate Model - Random Forest


###  Use Domain knowledge to remove one single trial (NPD_2020_4_17) avg Chlup at sensor detection limit (+- 0.1 ug/L). Leave other extreme values.
test1 = test %>% 
  dplyr::filter(round(Chl.up,2) > 0.1 | round(Chl.up,2) < -0.1)

####KN edit 9-21-2021, 5-fold cross validation (repeated 1000 times) to assess tuning parameters based on SSR objective measure of fit)

#Note, I'm manually playing around with these to find a sweet spot, not going to document progression.
#Goal isn't to find best model, goal is to find competitive model.

trees=200
try = 1
split=5
cp.par = .08

SSR.sim = rep(0,200)
seed.vec = 2001:2200 # giving same numbers
for(k in 1:200){

	SSR = 0
	set.seed = seed.vec[k]
	test.ind.matrix = matrix(sample(1:25),5,5)
	for(j in 1:5){
		test.subset = test1[test.ind.matrix[j,],]
		control.subset = test1[-test.ind.matrix[j,],]
	
		control.model.rf = 			randomForest(Clearance~Temp+Salinity+Turbidity+TPM+OC+Site	,
	                        data=control.subset, # use data	with extreme values removed 
                        ntree=trees,
                        mtry=try,
                        control=rpart.control(minsplit=		split,cp=cp.par))
		residuals = test1$Clearance[test.ind.matrix[j,]] - 		predict(control.model.rf,newdata=test.subset)
		SSR = SSR + sum(residuals^2)
	}
	SSR.sim[k] = SSR
}
mean(SSR.sim)

#mean SSR Value (cursory search only, not systematic).
#(trees,try,minsplit,cp) , mean(SSR)

#(100,3,2,.05) , 101.5m
#(200,3,2,.05) , 100.4m
#(50,3,2,.05) ,  108.5m
#(400,3,2,.05) , 106.1m

#Feel comfortable with 200 trees.

#(200,3,2,.05) , 100.4m
#(200,2,2,.05) , 97.8m
#(200,1,2,.05) , 92.3m
#(200,4,2,.05) , 104.4m

#Feel comfortable with mtry = 1

#(200,1,2,.05) , 92.3m
#(200,1,3,.05) , 84.5m
#(200,1,4,.05) , 91.9m
#(200,1,5,.05) , 97.9m

#(200,1,3,.05) , 84.5m
#(200,1,3,.02) , 90.0m
#(200,1,3,.08) , 89.3m

#Feel comfortable with cp=.05

#THEA, I only went through this once, if you want to spin your wheels you could go back through with 200,1,3,.05 as your starting point and redo ntrees, then mtry, then splits, then cp a second time (or even a third).  I think we've got most the low hanging fruit to be honest.  In general our original model was overfitting, most optimized parameters have been tuned towards less aggressive settings.

####KN edit end


############# Random Forest - WQ values affect on Filtration trails only #############

test2 <- test1[test1$Experiment=="Filtration",] # Filtration trials only

r.sq.vec = rep(0,1000)
rank.mat = data.frame(matrix(0,6,1001))
imp.mat = data.frame(matrix(0,6,1001))
rank.mat[,1] = c("OC","Salinity","Site","Temp","TPM","Turbidity")
imp.mat[,1] = c("OC","Salinity","Site","Temp","TPM","Turbidity")
for(i in 1:1000){
  
model.rf.filter = randomForest(Clearance~Temp+Salinity+Turbidity+TPM+OC+Site,
                               data=test2, # use data with extreme values removed 
                               ntree=100,
                               mtry=3,
                               control=rpart.control(minsplit=2,cp=.05))
model.rf.filter
# plot(model.rf.filter)
y.hat.filter = predict(model.rf.filter,newdata=test2) # model predicted values
res.filter = test2$Clearance - y.hat.filter # residuals - difference between measured values and model predicted values 
r.sq.filter = (var(test2$Clearance) - var(res.filter))/var(test2$Clearance)
r.sq.filter # r squared value for random forest model
### R^2: 0.646

r.sq.vec[i]=r.sq.filter

### Random Forest variable importance
Var_import <- importance(model.rf.filter)
Var_import <- Var_import[order(Var_import[,1], decreasing=TRUE),] # sort matrix highest to lowest - output names numeric
Var_import_df <- data.frame(as.list(Var_import)) # convert named numeric to data.frame

# dataframe with sorted importance values - highest to lowest
Var_import_df %<>% 
  pivot_longer(cols = c(Turbidity, Temp, TPM, Salinity, OC, Site), 
               names_to = "Variable", 
               values_to = "Importance") %>% 
  arrange(desc(Importance)) 

# convert importance values to percents
sum_import <- sum(Var_import_df$Importance)
Var_import_df %<>% 
  mutate(Importance_pct = (Importance / sum_import)*100) %>% 
  dplyr::select(-c(Importance))
Var_import_df$rank = 1:6
Var_import_df = Var_import_df[order(Var_import_df$Variable),]
imp.mat[,i+1] = Var_import_df$Importance_pct
rank.mat[,i+1] = as.numeric(Var_import_df$rank)

}
# alph order for variables
apply(rank.mat[,2:1001],1, mean)   

output = data.frame(variable = c("OC","Salinity","Site","Temp","TPM","Turbidity"),
                    rank_avg = apply(rank.mat[,2:1001],1,mean),
                    rank_sd = apply(rank.mat[,2:1001],1,sd),
                    imp_avg = apply(imp.mat[,2:1001],1,mean),
                    imp_sd = apply(imp.mat[,2:1001],1,sd))

output <- output %>% 
  arrange(rank_avg)

output_table <- xtable(output, label = NULL)


```

```{r FR_Site_boxplot, echo=FALSE, fig.keep = 'all', dev = 'pdf', fig.align= "center", fig.asp=0.618, out.width="70%", fig.width=6,  message= F, fig.cap="Box plots of habitat clearance rates (HCR) during filtration trials; control trials are listed in Table 1. Each data point is the mean of a single filtration trial. HCR was not statistically different among sites (one-way Kruskal-Wallis). Filtration trials were conducted between February 2018 to June 2019 at San Rafael, CA (restored \\textit{O. lurida} reefs); Morro Bay, CA (Morro Bay Oyster Company \\textit{C. gigas} aquaculture); and Newport Bay, CA (Shellmaker and Deanza, restored beds). \\label{FR_Site_boxplot}"}

## HCR averages by Site
FR_Site <- Filter_only_outRmd_data %>% 
  group_by(Site) %>% 
  summarize(avg_HCR = mean(L_Hr_m2),
            SD = sd(L_Hr_m2),
            n_samples = length(L_Hr_m2),
            median = median(L_Hr_m2),
            varience = var(L_Hr_m2))

#### one-way ANOVA -- HCR by Site
HCR_site_aov <- aov(L_Hr_m2 ~ Site, data = Filter_only_outRmd_data)
#summary(HCR_site_aov)
HCR_aov_values <- broom::tidy(HCR_site_aov)

# Post-Hoc Tukey test
tukey_HCR <- TukeyHSD(HCR_site_aov)
#tukey_HCR
#plot(tukey_HCR)
#Honestly Significant Difference test
hds_HCR <- HSD.test(HCR_site_aov, trt = "Site")
#hds_HCR

########### Power analysis - sensitivity power analysis 
# one-way ANOVA effect size (n^2). n^2 = treatmentSumSquares / TotalSumSquares (TreatSS + ResidualSS) 
d_HCR_site <- { HCR_aov_values[[1,3]] / (HCR_aov_values[[1,3]] + HCR_aov_values[[2,3]]) }

# power analysis for "balanced one-way analysis of variance tests"
# For now Ted says to use the mean of the different sample sizes
HCR_site_power <- pwr.anova.test(f = d_HCR_site, 
                                 k = 4,
                                 n = mean(FR_Site$n_samples), 
                                 sig.level = 0.05,
                                 power = NULL) # solve for this

# test if shellmaker extreme value was removed. SD is approximately equal among sites
x <- Filter_only_outRmd_data %>% 
  filter(L_Hr_m2 > -1000) %>% 
  group_by(Site) %>% 
  summarize(avg_L_hr_m2 = mean(L_Hr_m2),
            SD = sd(L_Hr_m2),
            n_samples = length(L_Hr_m2))

####### Try Kruskal- Wallis one-way analysis of variance 
## non-parametric equvilant to one-way ANOVA
# Assumtions 1) continuous response variable 2) Independent groups 3) Distributions have similar shapes

# K-W test - tidy converts to clean dataframe
KW_HCR_site <- broom::tidy(kruskal.test(L_Hr_m2 ~ Site, data = Filter_only_outRmd_data))

# Significant results can then use VVV pairwise comparison between groups
# pairwise.wilcox.test(PlantGrowth$weight, PlantGrowth$group, p.adjust.method = "BH")

### Filtration by site (Filtration Trials only)
FR_box <- ggplot((Filter_only_outRmd_data), aes(Site, L_Hr_m2)) +
  geom_boxplot(color = Site_colors) +
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = Hab_FR_Label)

plot(FR_box) 

```

Mean HCR (Habitat Cearance Rates) at San Rafael was `r round(mean(SR_filter_sum$L_Hr_m2), 3)` L hr^-1^ m^-2^ (*N* = `r nrow(SR_filter_sum)`, *SD* = `r round(sd(SR_filter_sum$L_Hr_m2),1)`), `r round(mean(MB_filter_sum$L_Hr_m2),1)` L hr^-1^ m^-2^ (*N* = to `r nrow(MB_filter_sum)`, *SD* = `r round(sd(MB_filter_sum$L_Hr_m2),1)`) at Morro Bay, 
`r round(mean(NPSM_filter_sum$L_Hr_m2),1)` L hr^-1^ m^-2^ (*N* = `r nrow(NPSM_filter_sum)`, *SD* = `r round(sd(NPSM_filter_sum$L_Hr_m2),1)`) at Shellmaker, 
and `r round(mean(NPD_filter_sum$L_Hr_m2),1)` L hr^-1^ m^-2^ (*N* = `r nrow(NPD_filter_sum)`, *SD* = `r round(sd(NPD_filter_sum$L_Hr_m2),1)`) at Deanza (Figure \ref{FR_Site_boxplot}).
There is not sufficient evidence to conclude that HCR was significantly different among sites (one-way Kruskal-Wallis, *p* = `r round(KW_HCR_site$p.value,2)`) (Figure \ref{FR_Site_boxplot}).
A random forest regression containing only filtration trials (*R*^2^ = `r round(r.sq.filter, 2)`) indicated that `r Var_import_df[[1,1]]` (`r Var_import_df[[1,2]]`\%) had the highest relative importance to the model, followed by `r Var_import_df[[2,1]]` (`r Var_import_df[[2,2]]`\%), `r Var_import_df[[3,1]]` (`r Var_import_df[[3,2]]`\%), `r Var_import_df[[4,1]]` (`r Var_import_df[[4,2]]`\%), `r Var_import_df[[5,1]]` (`r Var_import_df[[5,2]]`\%), and `r Var_import_df[[6,1]]` (`r Var_import_df[[6,2]]`\%).

`r output_table`

```{r Final_Summary_table, include=FALSE, results='asis', echo=F}

# code to construct summary table. table constructed in Overleaf. 

# Organize & clean summary table data
Final_table <- Master_analysis_table %>%
  mutate(Date = mdy(Date),
         Site = ifelse(Site == "San Rafael", "SR",
                       ifelse(Site == "Morro Bay", "MB",
                              ifelse(Site == "Shellmaker", "NPSM",
                                     ifelse(Site == "Deanza", "NPD",  "Pattern Not Found")))),
         Experiment = ifelse(grepl("Neg Control*", Master_analysis_table$Experiment), 
                             gsub("Neg Control*", "Ctrl", Master_analysis_table$Experiment), 
                             "Fltr"),
         L_hr_m2 = signif(L_Hr_m2, digits = 2),
         pcnt_Chl_rmvd = signif(pcnt_Chl_rmvd, digits = 3), ### Get rid of this 
         Temp_C_Up = signif(Temp_C_Up, digits = 4),
         Sal_ppt_Up = signif(Sal_ppt_Up, digits = 4),
         Turbidity_NTU_Up = signif(Turbidity_NTU_Up, digits = 2),
         Avg_TPM_mg_L = round(Avg_TPM_mg_L, digits = 4),
         Avg_OC_Ratio = round(Avg_OC_Ratio, digits = 2),
         Chl_ug_L_Up = signif(Chl_ug_L_up, digits = 1),
         Chl_ug_L_Down = round(Chl_ug_L_down, digits = 1),
         avg_depth_m = round(avg_depth_m, digits = 2),
         d_bw_sondes_m = round(d_bw_sondes_m, digits = 1),
         avg_m_sec = round(avg_m_hr/3600, 2)) %>%
  filter(Experiment == "Fltr") %>% 
  dplyr::select(Site, Date, Tide, L_hr_m2, Chl_ug_L_Up, Chl_ug_L_Down, Temp_C_Up, Sal_ppt_Up, 
                Turbidity_NTU_Up, Avg_TPM_mg_L, Avg_OC_Ratio, 
                avg_depth_m, d_bw_sondes_m, avg_m_sec) %>% 
 dplyr::arrange(Site, Date)


# arrange by site latitude
Final_table <- arrange(transform(Final_table, Site = factor(Site,levels = site_by_lat_abv)), Site)

```

## Particle Selection

```{r Part_select_GrayData_KevinCode}

library(sqldf)
#Read in data

#1. Gill Area Measurements.csv
#Contains information on 17 Gigas and Lorida Oysters.  
#Relevant information is physiology of the animals.

#2. Frequency of beads among individuals_equal volume_FINAL.csv
#Not the same individuals as in Gill Area Measurement
#Each row is number of particles of a given size eaten byu 
#a given animal.  Ignore 'prob' variable
#we don't remember what it does.
#Equal volume was total volume was the same among
#bead sizes.  


#3. Frequency of beads among individuals_equal concentration_FINAL.csv
#Not the same individuals as in Gill Area Measurement
#Each row is number of particles of a given size eaten byu 
#a given animal.  Ignore 'prob' variable
#we don't remember what it does.
#Equal concentration was frequency of bead sizes was the same among

#4. Particle Size Preference_equal concentration_20um bead dominating volume.csv
#Raw data for concentration?  Ask Matt why this data doesnt generate dataset 3?

data1 = read.csv("./Data/gray_data/Gill_Area_Measurements.csv",
                 h=T, check.names = F)
data2 = read.csv("./Data/gray_data/Frequency_of_beads_among_individuals_equal_volume_FINAL.csv", 
                 h=T, check.names = F)
data3 = read.csv("./Data/gray_data/Frequency_of_beads_among_individuals_equal_concentration_FINAL.csv",
                 h=T, check.names = F)

#Analysis of data1.

Results = data.frame(Variable = c("Tww (g)","Shell Height (mm)","Gill Area (um)","Area (um^2)","Dry Tissue Weight (g)"),
					Lurida_mean = apply(data1[data1$Species=="O. lurida",3:7],2,mean),
					Lurida_sd = apply(data1[data1$Species=="O. lurida",3:7],2,sd),
					Lurida_n = rep(length(data1$Id[data1$Species=="O. lurida"]),5),
					Gigas_mean = apply(data1[data1$Species=="C. gigas",3:7],2,mean),
					Gigas_sd = apply(data1[data1$Species=="C. gigas",3:7],2,sd),
					Gigas_n = rep(length(data1$Id[data1$Species=="C. gigas"]),5),
					t = rep(0,5),
					pval = rep(0,5))

Results$t = (Results$Lurida_mean - Results$Gigas_mean)/sqrt((Results$Lurida_sd^2/Results$Lurida_n) + (Results$Gigas_sd^2/Results$Gigas_n))
Results$pval = pt(-abs(Results$t),Results$Lurida_n[1] + Results$Gigas_n[1] - 2)*2

#Conclusion.  With limited data scope, can't really conclude that these species have noticibly 
#different physiology.

#Analysis of data2.

data2 = data2[data2$Level!="Total",]
data2$mass = as.numeric(data2$Level)*data2$Count
data2a = sqldf("select Species, Individual, sum(mass) as mass from data2 group by Species, Individual")


Results2 = data.frame(Species = c("O. lurida","C. gigas","Control"),
					Mean.Freq.3 = c(sum(data2$Count[data2$Species=="O. lurida" & data2$Level == 3])/6,
								   sum(data2$Count[data2$Species=="C. gigas" & data2$Level == 3])/6,
								   sum(data2$Count[data2$Species=="Control" & data2$Level == 3])),									
					Mean.Freq.6 = c(sum(data2$Count[data2$Species=="O. lurida" & data2$Level == 6])/6,
								   sum(data2$Count[data2$Species=="C. gigas" & data2$Level == 6])/6,
								   sum(data2$Count[data2$Species=="Control" & data2$Level == 6])),									
					Mean.Freq.12 = c(sum(data2$Count[data2$Species=="O. lurida" & data2$Level == 10])/6,
								   sum(data2$Count[data2$Species=="C. gigas" & data2$Level == 10])/6,
								   sum(data2$Count[data2$Species=="Control" & data2$Level == 10])),									
					Mean.Freq.20 = c(sum(data2$Count[data2$Species=="O. lurida" & data2$Level == 20])/6,
								   sum(data2$Count[data2$Species=="C. gigas" & data2$Level == 20])/6,
								   sum(data2$Count[data2$Species=="Control" & data2$Level == 20])),									
					Mean.Mass = c(mean(data2a$mass[data2a$Species=="O. lurida"]),
							     mean(data2a$mass[data2a$Species=="C. gigas"]),
							     sum(data2a$mass[data2a$Species=="Control"])),
					SD.Mass = c(sd(data2a$mass[data2a$Species=="O. lurida"]),
							     sd(data2a$mass[data2a$Species=="C. gigas"]),
							     sd(data2a$mass[data2a$Species=="Control"])),

					n.Mass = c(length(data2a$mass[data2a$Species=="O. lurida"]),
							     length(data2a$mass[data2a$Species=="C. gigas"]),
							     length(data2a$mass[data2a$Species=="Control"])))

t.num = Results2$Mean.Mass[1] - Results2$Mean.Mass[2]
t.den = sqrt((Results2$SD.Mass[1]^2/Results2$n.Mass[1]) + (Results2$SD.Mass[2]^2/Results2$n.Mass[2]))
t = t.num/t.den
pval = 2*pt(-abs(t),10)
#pval .045.  Mass consumption is different for O. Lurida and C. Gigas.

#What about pelle selectiont.  Weighted chi square test.
#Total sample size for chi-sq observed table will be number of pellets
#Row proportions however will be by overall mass (hence the weighted)
#Observed table is row total (pellets) times row proportions (weight distribution)
#I'm 90% sure this is the right way to deal with this problem.

Lurida.prop.a = Results2[1,2:5]*c(3,6,10,20)
Lurida.prop = Lurida.prop.a/sum(Lurida.prop.a)
Gigas.prop.a = Results2[2,2:5]*c(3,6,10,20)
Gigas.prop = Gigas.prop.a/sum(Gigas.prop.a)
n.Lurida = sum(data2$Count[data2$Species=="O. lurida"])
n.Gigas = sum(data2$Count[data2$Species=="C. gigas"])

observed = matrix(as.numeric(c(Lurida.prop*n.Lurida,Gigas.prop*n.Gigas)),ncol=4,byrow=T)
expected = matrix(0,nrow=2,ncol=4)
for(i in 1:2){
	for(j in 1:4){
		expected[i,j] = sum(observed[i,])*sum(observed[,j])/sum(observed)
	}
}

chi.sq = sum((observed - expected)^2/expected)
pval = 1-pchisq(chi.sq,3)
#pval is 0.  Pellet selection habits appear to be different.



#Analysis of data3.

data3 = data3[data3$Level!="Total",]
data3$mass = as.numeric(data3$Level)*data3$Count
data3a = sqldf("select Species, Individual, sum(mass) as mass from data3 group by Species, Individual")


Results3 = data.frame(Species = c("O. lurida","C. gigas","Suspension"),
					Mean.Freq.3 = c(sum(data3$Count[data3$Species=="O. lurida" & data3$Level == 3])/6,
								   sum(data3$Count[data3$Species=="C. gigas" & data3$Level == 3])/6,
								   sum(data3$Count[data3$Species=="Suspension" & data3$Level == 3]))/7,									
					Mean.Freq.6 = c(sum(data3$Count[data3$Species=="O. lurida" & data3$Level == 6])/6,
								   sum(data3$Count[data3$Species=="C. gigas" & data3$Level == 6])/6,
								   sum(data3$Count[data3$Species=="Suspension" & data3$Level == 6]))/7,									
					Mean.Freq.12 = c(sum(data3$Count[data3$Species=="O. lurida" & data3$Level == 10])/6,
								   sum(data3$Count[data3$Species=="C. gigas" & data3$Level == 10])/6,
								   sum(data3$Count[data3$Species=="Suspension" & data3$Level == 10]))/7,									
					Mean.Freq.20 = c(sum(data3$Count[data3$Species=="O. lurida" & data3$Level == 20])/6,
								   sum(data3$Count[data3$Species=="C. gigas" & data3$Level == 20])/6,
								   sum(data3$Count[data3$Species=="Suspension" & data3$Level == 20]))/7,									
					Mean.Mass = c(mean(data3a$mass[data3a$Species=="O. lurida"]),
							     mean(data3a$mass[data3a$Species=="C. gigas"]),
							     sum(data3a$mass[data3a$Species=="Suspension"])),
					SD.Mass = c(sd(data3a$mass[data3a$Species=="O. lurida"]),
							     sd(data3a$mass[data3a$Species=="C. gigas"]),
							     sd(data3a$mass[data3a$Species=="Suspension"])),

					n.Mass = c(length(data3a$mass[data3a$Species=="O. lurida"]),
							     length(data3a$mass[data3a$Species=="C. gigas"]),
							     length(data3a$mass[data3a$Species=="Suspension"])))

t.num = Results3$Mean.Mass[1] - Results3$Mean.Mass[2]
t.den = sqrt((Results3$SD.Mass[1]^2/Results3$n.Mass[1]) + (Results3$SD.Mass[2]^2/Results3$n.Mass[2]))
t = t.num/t.den
pval = 2*pt(-abs(t),10)
#pval .76.  Mass consumption is may not different for O. Lurida and C. Gigas.
#Why are they eating so much more in this second experiment?

#What about pelle selectiont.  Weighted chi square test.
#Total sample size for chi-sq observed table will be number of pellets
#Row proportions however will be by overall mass (hence the weighted)
#Observed table is row total (pellets) times row proportions (weight distribution)
#I'm 90% sure this is the right way to deal with this problem.

Lurida.prop.a = Results3[1,2:5]*c(3,6,10,20)
Lurida.prop = Lurida.prop.a/sum(Lurida.prop.a)
Gigas.prop.a = Results3[2,2:5]*c(3,6,10,20)
Gigas.prop = Gigas.prop.a/sum(Gigas.prop.a)
n.Lurida = sum(data3$Count[data3$Species=="O. lurida"])
n.Gigas = sum(data3$Count[data3$Species=="C. gigas"])

observed = matrix(as.numeric(c(Lurida.prop*n.Lurida,Gigas.prop*n.Gigas)),ncol=4,byrow=T)
expected = matrix(0,nrow=2,ncol=4)
for(i in 1:2){
	for(j in 1:4){
		expected[i,j] = sum(observed[i,])*sum(observed[,j])/sum(observed)
	}
}

chi.sq = sum((observed - expected)^2/expected)
pval = 1-pchisq(chi.sq,3)
#pval is 0.0169.  Pellet selection habits appear to be different.

```


Final particle selection mosaic plots are in file "Mosaic plots and Rgooglemaps.R" in publication dropbox folder. Move code to this project eventually

```{r Particle_volume_mosaic_plot, echo=FALSE, fig.keep = 'all', dev = 'pdf', fig.align= "center", fig.asp=0.618, out.width="70%", fig.width=6,  message= F, fig.cap="Caption Needed From Matt", eval= F}
# select columns for mosaic plot
mosaic_table2 <- Results2[,1:5]
# rearrange data to long format and pull out data from column names
mosaic_table2 %<>% 
  pivot_longer(2:5, "particle_size_um", names_prefix = "Mean.Freq.", values_to = "mean_freq")
# convert particle size and species to discrete factors
mosaic_table2$particle_size_um <- factor(mosaic_table2$particle_size_um, levels = c(3, 6, 12, 20))
# convert species factor to character
mosaic_table2$Species <- as.character(mosaic_table2$Species)
# replace gigas with magallana
mosaic_table2 %<>%
  mutate(Species = replace(Species, Species %in% c("C. gigas"), "M. gigas"))

# moasic plots 
moasic_colors  <- c("M. gigas" = "#6D6340",
                    "O. lurida" = "#9986A5",
                    "Control" = "black")
part_label <- expression(paste("Particle Size Groups(", mu, "m)"))

moasic_plot2 <- ggplot(mosaic_table2) +
  geom_mosaic(aes(x = product(Species, particle_size_um), 
                  fill = Species, 
                  weight = mean_freq),
              offset = 0.01,
              alpha = 1) +
  scale_fill_manual(values = moasic_colors) +
  labs(y = "", 
       x = part_label, 
       title = "Frequencey of Artifical Particles Among Individuals",
       subtitle = "Equal Volume of Particle Sizes") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "right",
        panel.background = element_blank(),
        legend.text = element_text(face = "italic")
        )
moasic_plot2
```


```{r Particle_conc_mosaic_plot, echo=FALSE, fig.align="center", fig.asp=0.618, fig.width=6, warning= F, message= F, fig.keep='all', dev='pdf', out.width="70%", fig.cap="Caption Needed From Matt", eval= F}

# select columns for mosaic plot
mosaic_table3 <- Results3[,1:5]
# rearrange data to long format and pull out data from column names
mosaic_table3 <- mosaic_table3 %>% 
  pivot_longer(2:5, "particle_size_um", names_prefix = "Mean.Freq.", values_to = "mean_freq")
# convert particle size and species to discrete factors
mosaic_table3$particle_size_um <- factor(mosaic_table3$particle_size_um, levels = c(3, 6, 12, 20))
# convert species factor to character for replacing
mosaic_table3$Species <- as.character(mosaic_table3$Species)

mosaic_table3 <- mosaic_table3 %>% 
  mutate(Species = replace(Species, Species == "C. gigas", "M. gigas"),
         Species = replace(Species, Species == "Suspension", "Control"))

library(ggmosaic) # moasic plots 
moasic_colors <- Species_colors <- c("M. gigas" = "#6D6340",
                                     "O. lurida" = "#9986A5",
                                     "Control" = "black")
part_label <- expression(paste("Particle Size Groups(", mu, "m)"))

moasic_plot3 <- ggplot(mosaic_table3) +
  geom_mosaic(aes(x = product(Species, particle_size_um), 
                  fill = Species, 
                  weight = mean_freq),
              offset = 0.01,
              alpha = 1) +
  scale_fill_manual(values = moasic_colors) +
  labs(y = "", 
       x = part_label, 
       title = "Frequencey of Artifical Particles Among Individuals",
       subtitle = "Equal Concentration of Particle Sizes") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "right",
        panel.background = element_blank(),
        legend.text = element_text(face = "italic")
        )
moasic_plot3
```


## Allometric Predictor of Biomass

Pairwise parameter comparison of models relating allometric predictors of biomass
reveal that there are statistically significant differences between each of the
three observed O. lurida populations (San Diego, CA, Deanza, CA, Yaquina
Bay, OR). Most notably, evidence supports that animals observed in Yaquina
Bay, in general, are longer and proportionally support higher dry tissue weight
than species found at the Deanza and San Diego sites. Though there are less
notable yet still significant differences between the populations sampled at the
two Southern California sites.

```{r Oly_DTW_Shell_length_setup, include=FALSE}
############ Olympia DTW and Shell Length #####################
# Determine what quadrat samples have missing data (NAs)
NPD_biv_bio_summary <- NPD_bivalves %>% 
  group_by(excavation_sample) %>% 
  summarize(avg_DTW = mean(tissue_dry_weight_g))

# what are the unique species in dataset
#unique(NPD_bivalves$species)

#Newport, CA All excavation quadrat samples processed to date aggregated (summer 2020)

# Still contains missing data --> which quadrats to use for analyses
NPD_bivalve_data_clean <- NPD_bivalves %>% 
  filter(!excavation_sample %in% c("Q10", "Q4", "Q8")) %>% 
  mutate(species = ifelse(grepl("adula", ignore.case = T, species), "Adula diegensis",
                          ifelse(grepl("muscul.", ignore.case = T, species), "Musculista senhousia",
                                 ifelse(grepl("myt", ignore.case = T, species), "Mytilus galloprovincialis",
                                        ifelse(grepl(".lurida", ignore.case = T, species), "Ostrea lurida",
                                               ifelse(grepl("mussel", ignore.case = T, species), "Unknown mussel",
                                                      ifelse(grepl(".spinosum", ignore.case = T, species), "Crucibulum spinosum",
                                                       "Pattern Not Found"))))))) %>% 
  # Filter out unclassified/ empty species & samples less than 0g (processing error, doesn't make sense)
  filter(!species %in% c("Pattern Not Found", "Crucibulum spinosum"))

# Remove outlier and negative DTW value
NPD_bivalve_outrmd <- NPD_bivalve_data_clean %>%
  mutate(DTW_outlier = find_outlier(NPD_bivalve_data_clean$tissue_dry_weight_g)) %>% 
  filter(DTW_outlier == FALSE & tissue_dry_weight_g >= 0)

# All excavation quadrat samples processed to date aggregated (summer 2020)
# filter only O. lurida dtw in newport
NPD_oly_outRmd <- NPD_bivalve_outrmd %>% 
  filter(species == "Ostrea lurida") %>% 
  drop_na() %>% 
  as_tibble()
```

```{r Oly_DTW_SL_NP_YQ_SD_analysis_figure}
## import Matt Gray DTW dataset - Yaquina Bay, Oregon
## This code chunk needs "Oly_DTW_Shell_length_setup" chunk above 
Gray_Oly_Data <- fread(file.path('./Data/bivalve_density_community', "MattGray_2011_2013_Olurida_allometric_data.csv"))
colnames(Gray_Oly_Data) <- c("ID", "Date", "species", "length_mm", "shell_tissue_g", "vial_weight", "Vial_wet_oyster", "Tissue_weight_g", "vial_dry_oyster", "tissue_dry_weight_g")
Gray_Oly_Data %<>% 
  mutate(species = ifelse(species %in% "O. lurida", "Ostrea lurida", "Pattern Not FOund"),
         Site = "Yaquina")

Gray_Oly_SL_DTW <- Gray_Oly_Data %>% 
  dplyr::select(Date, Site, species, length_mm, tissue_dry_weight_g)

##### Import Jason Langevin DTW dataset - San Diego Bay, California
SD_Oly_SL_DTW <- read_csv("./Data/bivalve_density_community/Langevin_SD_O_lurida_length_weight_data.csv")
SD_Oly_SL_DTW %<>% 
  rename("Tidal_hight" = "Tidal height",
         "length_mm" = "Length (mm)",
         "tissue_dry_weight_g" = "Dy (g)") %>% 
  mutate("species" = "Ostrea lurida") %>% 
  dplyr::select(Site, species, length_mm, tissue_dry_weight_g)

#### Import Newport Deanza (NPD) O. lurida dataframe cleaned above

NPD_Oly_SL_DTW <- NPD_oly_outRmd %>% 
  mutate(Date = excavation_date) %>% 
  dplyr::select(Date, Site, species, length_mm, tissue_dry_weight_g)

# join Newport, Yaquina, and San Diego data together into singe dataframe
Oly_combo_DTW_SL <- full_join(NPD_Oly_SL_DTW,Gray_Oly_SL_DTW) %>% 
  full_join(SD_Oly_SL_DTW) %>% 
  mutate(legend = case_when(Site == "Deanza" & species == "Ostrea lurida" ~ "Deanza",
                            Site == "Yaquina" & species == "Ostrea lurida" ~ "Yaquina",
                            Site == "San Diego" & species == "Ostrea lurida" ~ "San Diego"))

#Oly_combo_DTW_SL$legend <- fct_relevel(Oly_combo_DTW_SL$legend, "Yaquina", "San Diego", "Deanza")
Oly_combo_DTW_SL <- Oly_combo_DTW_SL %>% 
  mutate(Site = fct_relevel(Site, "San Diego", "Deanza", "Yaquina"))

#### start of Kevin's code ###
#find domain of shell length

#fit separate models log(weight)~log(length)

#come up with 1000 evenly separated x values over domain.  Compute y.hat for each of the three models
#add to plot.

temp = Oly_combo_DTW_SL

#find domain of shell lenght

x.seq = seq(min(temp$length_mm),max(temp$length_mm),by=0.05)

#fit separate models

x1 = temp$length_mm[temp$Site == "Deanza"]
y1 = temp$tissue_dry_weight_g[temp$Site=="Deanza"]
x2 = temp$length_mm[temp$Site == "San Diego"]
y2 = temp$tissue_dry_weight_g[temp$Site=="San Diego"]
x3 = temp$length_mm[temp$Site == "Yaquina"]
y3 = temp$tissue_dry_weight_g[temp$Site=="Yaquina"]

lx1 = log(x1)
lx2 = log(x2)
lx3 = log(x3)
ly1 = log(y1)
ly2 = log(y2)
ly3 = log(y3)

m1 = lm(ly1~lx1)
m2 = lm(ly2~lx2)
m3 = lm(ly3~lx3)

#example with confidence bounds
#y.seq1 = exp(predict(m1,newdata=data.frame(lx1=log(x.seq)),interval="confidence"))
y.seq1 = exp(predict(m1,newdata=data.frame(lx1=log(x.seq))))
y.seq2 = exp(predict(m2,newdata=data.frame(lx2=log(x.seq))))
y.seq3 = exp(predict(m3,newdata=data.frame(lx3=log(x.seq))))

temp1 = data.frame(x.seq,y.seq1)
temp2 = data.frame(x.seq,y.seq2)
temp3 = data.frame(x.seq,y.seq3)
### End of Kevin's code ####

######## Graph with Deanza, Yaquina & San Diego DTW x Shell length

Oly_combo_DTW_SL <- Oly_combo_DTW_SL %>% 
  arrange(Site, "San Diego", "Deanza", "Yaquina")

Combo_Oly_shell_DTW_figure <- ggplot(Oly_combo_DTW_SL, aes(length_mm, tissue_dry_weight_g)) +
  geom_point(aes(color = Site, shape = Site)) +
  scale_color_manual(labels = c("San Diego Bay, CA",
                                "Deanza, Newport Bay, CA",
                                "Yaquina Bay, OR"),
                     values = c("San Diego" = "#FCA636FF",
                                "Deanza" = "#440154FF",
                                "Yaquina" = "#26828EFF"), #9986A5
                     na.translate = TRUE, drop = FALSE) + 
  scale_shape_manual(labels = c("San Diego Bay, CA",
                                "Deanza, Newport Bay, CA",
                                "Yaquina Bay, OR"),
                     values = c("San Diego" = 6,
                                "Yaquina" = 3,
                                "Deanza" = 16),
                     na.translate = TRUE, drop = FALSE) + 
  theme_classic() +
  theme(legend.box.background = element_rect(colour = "black"),
        legend.justification = c(0, 1), 
        legend.position = c(0.075, 0.95),
        legend.title = element_blank()) +
  ylim(0,1)+
  labs(x = 'Shell Length (mm)',
       y = 'Dry Tissue Weight (g)')+
  geom_line(data=temp1,aes(x.seq,y.seq1), color = "#440154FF") + #9986A5
  geom_line(data=temp2,aes(x.seq,y.seq2), color = "#FCA636FF") +
  geom_line(data=temp3,aes(x.seq,y.seq3), color = "#26828EFF") #20A386FF


Combo_Oly_shell_DTW_figure

ggsave(Combo_Oly_shell_DTW_figure, height=3.708, width=6, filename = "Oly_DTW_SL_allsites.png")
#fig.asp=0.618

```



# Supplemental / Repository 

```{r Site WQ ANOVA, fig.width=7, fig.asp=1.2, fig.cap="Box plots of ambient (upstream) A) temperature, B) salinity, C) turbidity, D) total particulate matter, E) organic content, and F) chlorophyll $\\alpha$ from filtration trials. One-way ANOVAs compared the difference between water quality variables and site. Significantly different results were grouped by a post-hoc Tukey's HSD; significantly different sites do not share a common letter, and non-significant differences share letters. Site effects on OC were significant, and a Newman-Keuls post-hoc analysis determined a significant difference between San Rafael and Deanza undetected by Tukey's HSD. Trials were conducted from February 2018 to June 2019 at San Rafael, CA (restored reefs); Morro Bay, CA (Morro Bay Oyster Company, aquaculture); and Newport Bay, CA (Shellmaker and Deanza, restored beds). \\label{Site_WQ_boxplot}" }

###### Temperature 
#hist(Filter_only_outRmd_data$Temp_C_Up)
site_temp_aov <- aov(Temp_C_Up ~ Site, data = Filter_only_outRmd_data)
#summary(site_temp_aov)
temp_aov_values <- broom::tidy(site_temp_aov) # put ANOVA model into tibble to extract individual values
# Post-Hoc Tukey test
tukey_temp <- TukeyHSD(site_temp_aov)
#tukey_temp
#plot(tukey_temp)

#Honestly Significant Difference test --> Don't need if non-significant ANOVA
hds_temp <- HSD.test(site_temp_aov, trt = "Site")
#hds_temp

Temp_box <- ggplot(Filter_only_outRmd_data, aes(Site, Temp_C_Up)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = expression(paste("Temperature (",degree, "C)")),
       title = "A)")

####### Salinity
#hist(site_data_outrmd$Sal_ppt_Up)
site_sal_aov <- aov(Sal_ppt_Up ~ Site, data = Filter_only_outRmd_data)
sal_aov_values <- broom::tidy(site_sal_aov)

# Post-Hoc Tukey test
tukey_sal <- TukeyHSD(site_sal_aov)
#tukey_sal
#plot(tukey_sal)

#Honestly Significant Difference test
hds_sal <- HSD.test(site_sal_aov, trt = "Site")
#hds_sal

Sal_box <- ggplot(Filter_only_outRmd_data, aes(Site, Sal_ppt_Up)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = "Salinity (ppt)",
       title = "B)") +
  annotate("text", "San Rafael", 
           max((site_data_outrmd %>% 
                  filter(Site == "San Rafael"))$Sal_ppt_Up) + 0.75, label = "a") +
  annotate("text", "Morro Bay", 
           max((site_data_outrmd %>% 
                  filter(Site == "Morro Bay"))$Sal_ppt_Up) + 0.75, label = "b") +
  annotate("text", "Shellmaker", 
           max((site_data_outrmd %>% 
                  filter(Site == "Shellmaker"))$Sal_ppt_Up) + 0.75, label = "b") +
  annotate("text", "Deanza", 
           max((site_data_outrmd %>% 
                  filter(Site == "Deanza"))$Sal_ppt_Up) + 0.75, label = "b") 

######### Turbidity
#hist(site_data_outrmd$Turbidity_NTU_Up)
site_turb_aov <- aov(Turbidity_NTU_Up ~ Site, data = Filter_only_outRmd_data)
turb_aov_values <- broom::tidy(site_turb_aov)

# Post-Hoc Tukey test
tukey_turb <- TukeyHSD(site_turb_aov)
#tukey_turb
#plot(tukey_turb)

#Honestly Significant Difference test
hds_turb <- HSD.test(site_turb_aov, trt = "Site")
#hds_turb

Turb_box <- ggplot(Filter_only_outRmd_data, aes(Site, Turbidity_NTU_Up)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = "Turbidity (NTU)",
       title = "C)") +
  annotate("text", "San Rafael", 
           max((site_data_outrmd %>% 
                  filter(Site == "San Rafael"))$Turbidity_NTU_Up) + 2, 
           label = "a") +
  annotate("text", "Morro Bay", 
           max((site_data_outrmd %>% 
                  filter(Site == "Morro Bay"))$Turbidity_NTU_Up) + 2, 
           label = "b") +
  annotate("text", "Shellmaker", 
           max((site_data_outrmd %>% 
                  filter(Site == "Shellmaker"))$Turbidity_NTU_Up) + 2, 
           label = "c") +
  annotate("text", "Deanza", 
           max((site_data_outrmd %>% 
                  filter(Site == "Deanza"))$Turbidity_NTU_Up) + 2, 
           label = "c") 

####### TPM
#hist(site_data_outrmd$Avg_TPM_mg_L)
site_TPM_aov <- aov(Avg_TPM_mg_L ~ Site, data = Filter_only_outRmd_data)
TPM_aov_values <- broom::tidy(site_TPM_aov)

# Post-Hoc Tukey test
tukey_tpm <- TukeyHSD(site_TPM_aov)
#tukey_tpm
#plot(tukey_tpm)

#Honestly Significant Difference test
hds_tpm <- HSD.test(site_TPM_aov, trt = "Site")
#hds_tpm

Tpm_box <- ggplot(Filter_only_outRmd_data, aes(Site, Avg_TPM_mg_L)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = expression(paste("Total Particulate Matter (mg/L)")),
       title = "D)") +
  annotate("text", "San Rafael", 
           max((site_data_outrmd %>% 
                  filter(Site == "San Rafael"))$Avg_TPM_mg_L) + 10, label = "a") +
  annotate("text", "Morro Bay", 
           max((site_data_outrmd %>% 
                  na.omit() %>% 
                  filter(Site == "Morro Bay"))$Avg_TPM_mg_L) + 10, label = "b") +
  annotate("text", "Shellmaker", 
           max((site_data_outrmd %>% 
                  filter(Site == "Shellmaker"))$Avg_TPM_mg_L) + 10, label = "b") +
  annotate("text", "Deanza", 
           max((site_data_outrmd %>% 
                  na.omit() %>% 
                  filter(Site == "Deanza"))$Avg_TPM_mg_L) + 10, label = "b") 

####### OC
#hist(site_data_outrmd$Avg_OC_Ratio)
site_OC_aov <- aov(Avg_OC_Ratio ~ Site, data = Filter_only_outRmd_data %>% 
                     filter(!is.na(Avg_OC_Ratio)))
OC_aov_values <- broom::tidy(site_OC_aov)

# Post-Hoc Tukey test
tukey_oc <- TukeyHSD(site_OC_aov)

#Honestly Significant Difference test
hds_oc <- HSD.test(site_OC_aov, trt = "Site")
# Not not showing differences between sites

# Less conservative Post-hoc analysis - Newman-keuls (also good for simple group structure)
library(DescTools)
NK_oc <- PostHocTest(site_OC_aov, method = "newmankeuls", conf.level = 0.95)

OC_box <- ggplot(Filter_only_outRmd_data, aes(Site, Avg_OC_Ratio)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = "Organic Content (Ratio)",
       title = "E)")

###### Chlorphyll
#hist(site_data_outrmd$Chl_ug_L_up)
site_chl_aov <- aov(Chl_ug_L_up ~ Site, data = Filter_only_outRmd_data)
#summary(site_chl_aov)
chl_aov_values <- broom::tidy(site_chl_aov) # put ANOVA model into tibble to extract individual values
# Post-Hoc Tukey test
tukey_chl <- TukeyHSD(site_chl_aov, ordered = FALSE, conf.level = 0.95)
#tukey_chl
#plot(tukey_chl)

#Honestly Significant Difference test
hds_chl <- HSD.test(site_chl_aov, trt = "Site")
#hds_chl

Chl_box <- ggplot(Filter_only_outRmd_data, aes(Site, Chl_ug_L_up)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = expression(paste("Chlorophyll ",alpha, " (", mu,"g/L)")),
       title = "F)") 

grid.arrange(Temp_box, Sal_box, Turb_box, Tpm_box, OC_box, Chl_box,nrow=3)
```


Ambient water quality during filtration trials varied within and among sites (Figure \ref{Site_WQ_boxplot}). 
Salinity was significantly different among sites as determined by a one-way ANOVA at a *p* < 0.05 (*F*(`r sal_aov_values[[1,2]]`, `r sal_aov_values[[2,2]]`) = `r round(sal_aov_values[[1,5]],2)`, *p* < 0.001), along with turbidity (*F*(`r turb_aov_values[[1,2]]`, `r turb_aov_values[[2,2]]`) = `r round(turb_aov_values[[1,5]],2)`, *p* < 0.001), and TPM (*F*(`r TPM_aov_values[[1,2]]`, `r TPM_aov_values[[2,2]]`) = `r round(TPM_aov_values[[1,5]],2)`, *p* = < 0.001) (Figure \ref{Site_WQ_boxplot}).  
Temperature (*F*(`r temp_aov_values[[1,2]]`, `r temp_aov_values[[2,2]]`) = `r round(temp_aov_values[[1,5]],2)`, *p* = `r format(round(temp_aov_values[[1,6]],2), nsmall = 2)`), and Chl $\alpha$ (*F*(`r chl_aov_values[[1,2]]`, `r chl_aov_values[[2,2]]`) = `r round(chl_aov_values[[1,5]],2)`, *p* = `r round(chl_aov_values[[1,6]],2)`) were not different among sites (Figure \ref{Site_WQ_boxplot}).
OC was significant among sites (*F*(`r OC_aov_values[[1,2]]`, `r OC_aov_values[[2,2]]`) = `r round(OC_aov_values[[1,5]],2)`, *p* = `r round(OC_aov_values[[1,6]],2)`), but the post-hoc Tukey HSD did not reveal significant differences among sites. 
Therefore, I use a less conservative post-hoc analysis, the Newman-Keuls method, and found that OC was significantly different between Shellmaker and Deanza (*p* = `r round(NK_oc$Site[[3,4]],2)`).

```{r ChlRmd_Site_boxplot, echo=FALSE, fig.keep = 'all', dev = 'pdf', fig.align= "center", fig.asp=0.618, out.width="70%", message= F, fig.cap="Box plots of percent chlorophyll $\\alpha$ removal (Chl\\textsubscript{up} - Chl\\textsubscript{down} / Chl\\textsubscript{up} * 100) during filtration trials, control trials are listed in Table 1. Each data point is the mean of a single filtration trial. Filtration trials were conducted between February 2018 to June 2019 at San Rafael, CA (restored reefs); Morro Bay, CA (Morro Bay Oyster Company aquaculture); and Newport Bay, CA (Shellmaker and Deanza, restored beds). \\label{ChlRmd_Site_boxplot}"}

### Chl removed summary - filtration trials only
Chlrmd_Site <- Filter_only_outRmd_data %>% 
  group_by(Site) %>% 
  summarize(avg_Chlrmd = mean(pcnt_Chl_rmvd),
            SD = sd(pcnt_Chl_rmvd),
            n_samples = length(pcnt_Chl_rmvd))

Chlrmd_site_aov <- aov(pcnt_Chl_rmvd ~ Site, data = Filter_only_outRmd_data)
#summary(HCR_site_aov)
Chlrmd_aov_values <- broom::tidy(Chlrmd_site_aov)

# Post-Hoc Tukey test
tukey_Chlrmd <- TukeyHSD(Chlrmd_site_aov)
#tukey_Chlrmd
#plot(tukey_Chlrmd)
#Honestly Significant Difference test
hds_Chlrmd <- HSD.test(Chlrmd_site_aov, trt = "Site")
#hds_Chlrmd

#### exteme Shellmaker may violate ANOVA equal variance assumption
## more suited for Kruskal-Wallis
KW_Chlrmd_site <- broom::tidy(kruskal.test(pcnt_Chl_rmvd ~ Site, data = Filter_only_outRmd_data))


########### Power analysis - sensitivity power analysis 
# one-way ANOVA effect size (n^2). n^2 = treatmentSumSquares / TotalSumSquares (TreatSS + ResidualSS) 
d_Chlrmd_site <- {Chlrmd_aov_values[[1,3]] / (Chlrmd_aov_values[[1,3]] + Chlrmd_aov_values[[2,3]]) }

# power analysis for "balanced one-way analysis of variance tests"
# For now Ted says to use the mean of the different sample sizes
Chlrmd_site_power <- pwr.anova.test(f = d_Chlrmd_site, 
                                 k = 4,
                                 n = mean(Chlrmd_Site$n_samples), 
                                 sig.level = 0.05,
                                 power = NULL) # solve for this

Chl_Rmd_box <- ggplot(Filter_only_outRmd_data, aes(Site, pcnt_Chl_rmvd)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.5, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = Prt_Chl_Label)

plot(Chl_Rmd_box)

```

## Percent Chlorophyll $\alpha$ Removal

The mean percent Chl $\alpha$ removal at the San Rafael site was `r round(mean(SR_filter_sum$pcnt_Chl_rmvd),1)`\% (*N* = `r nrow(SR_filter_sum)`, *SD* = `r round(sd(SR_filter_sum$pcnt_Chl_rmvd),2)`) (Figure \ref{ChlRmd_Site_boxplot}). 
Filtration trials at Morro Bay had a mean Chl $\alpha$ removal of `r round(mean(MB_filter_sum$pcnt_Chl_rmvd),1)`\% (*N* = `r nrow(MB_filter_sum)`, *SD* = `r round(sd(MB_filter_sum$pcnt_Chl_rmvd),1)`). 
At Deanza, mean Chl $\alpha$ removal was `r round(mean(NPD_filter_sum$pcnt_Chl_rmvd),1)`\% (*N* = `r nrow(NPD_filter_sum)`, *SD* =  `r round(sd(NPD_filter_sum$pcnt_Chl_rmvd),1)`).
Mean Shellmaker Chl $\alpha$ removal was `r round(mean(NPSM_filter_sum$pcnt_Chl_rmvd),1)` \% (*N* = `r nrow(NPSM_filter_sum)`, *SD* = `r round(sd(NPSM_filter_sum$pcnt_Chl_rmvd),1)`) (Figure \ref{ChlRmd_Site_boxplot}).
Chl $\alpha$ removal in filtration trials did not differ significantly between sites (one-way Kruskal-Wallis, *p* = `r round(KW_Chlrmd_site$p.value,2)`).

```{r HCR_Quantile_Regression, eval=FALSE, fig.width=7, fig.asp=1.2, warning=F, message = F, fig.cap="The relationship between water quality variables and habitat clearance rate (HCR). For all oyster habitat filtration trials, quantile regression with $\\tau$ = 0.5 and $\\tau$ = 0.9 was used to test wheather HCR changed with A) temperature, B) salinity, C) turbidity, D) total particulate matter, or E) organic content; slopes that are not significantly different from zero are indicated by red dashed lines. Dotted gray lines are the mean value of HCR. \\label{Quantile_Reg}"}
#create model variables
attach(Filter_only_outRmd_data)
qr_x <- cbind(Temp_C_Up, Sal_ppt_Up, Turbidity_NTU_Up, Avg_TPM_mg_L, Avg_OC_Ratio, Site)
#summary(qr_x)
qr_y <- cbind(L_Hr_m2)
#summary(qr_y)

## OLS Model
HCR_lm <- lm(qr_y ~ qr_x)
#summary(HCR_lm)
#hist(Filter_only_outRmd_data$L_Hr_m2)
# Look at linear model assumptions
# Doesn't look good with this data
#par(mfrow=c(2,2))
#plot(HCR_lm)

##########################################
############## Quantile Regression ###############
## Bivariate comparisons only, not doing multivariate in this study (Talked with Ted Grosholz)
library(quantreg) # Quantile Rregression package

#### Look at change over most Quantiles
qr_temp <- rq(L_Hr_m2 ~ Temp_C_Up,
             tau = seq(0.05,0.95, by = 0.05),
             na.action = na.omit,
             data = Filter_only_outRmd_data)
#summary(qr_temp)
#plot(qr_temp)

#### Temperature
## Looking at .5 and .9 quantiles of Habitat Clearance Rates - Filtration most likely on
qr_5_9_temp <- rq(L_Hr_m2 ~ Temp_C_Up,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRtemp <- summary(qr_5_9_temp, se = "boot", R = 1000)
#sumQRtemp

## Graph
# if statements choose linetype - solid (significant) or dashed (not significant)
qrplot_temp <- ggplot(Filter_only_outRmd_data, aes(Temp_C_Up, L_Hr_m2)) +
  geom_point(aes(color = Site)) +
  # gray line indicating mean of HCR
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_Hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
  labs(x = expression(paste("Temperature (", degree, "C)")),
       y = "",
       title = "A)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRtemp[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRtemp[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRtemp[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRtemp[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRtemp[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRtemp[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

### Salinity 
qr_5_9_sal <- rq(L_Hr_m2 ~ Sal_ppt_Up,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRsal <- summary(qr_5_9_sal, se = "boot", R = 1000)
#sumQRsal

# Graph
qrplot_sal <- ggplot(Filter_only_outRmd_data, aes(Sal_ppt_Up, L_Hr_m2)) +
  geom_point(aes(color = Site)) +
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_Hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
    labs(x = expression(paste("Salinity (ppt)")),
       y = "",
       title = "B)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRsal[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRsal[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRsal[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRsal[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRsal[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRsal[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

### Turbidity
qr_5_9_turb <- rq(L_Hr_m2 ~ Turbidity_NTU_Up,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRturb <- summary(qr_5_9_turb, se = "boot", R = 1000)
#sumQRturb

# Graph
qrplot_turb <- ggplot(Filter_only_outRmd_data, aes(Turbidity_NTU_Up, L_Hr_m2)) +
  geom_point(aes(color = Site)) +
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_Hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
  labs(x = expression(paste("Turbidity (NTU)")),
       y = "",
       title = "C)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRturb[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRturb[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRturb[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRturb[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRturb[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRturb[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

### TPM
qr_5_9_tpm <- rq(L_Hr_m2 ~ Avg_TPM_mg_L,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRtpm <- summary(qr_5_9_tpm , se = "boot", R = 1000)
#sumQRtpm

# Graph
qrplot_tpm <- ggplot(Filter_only_outRmd_data, aes(Avg_TPM_mg_L, L_Hr_m2)) +
  geom_point(aes(color = Site)) +
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_Hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
  labs(x = expression(paste("Total Particulate Matter (", mu,"g/L)")),
       y = "",
       title = "D)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRtpm[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRtpm[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRtpm[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRtpm[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRtpm[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRtpm[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

### OC 
qr_5_9_oc <- rq(L_Hr_m2 ~ Avg_OC_Ratio,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRoc <- summary(qr_5_9_oc , se = "boot", R = 1000)
#sumQRoc

# Graph
qrplot_oc <- ggplot(Filter_only_outRmd_data, aes(Avg_OC_Ratio, L_Hr_m2)) +
  geom_point(aes(color = Site)) +
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_Hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
  labs(x = expression(paste("Organic Content (ratio)")),
       y = "",
       title = "E)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRoc[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRoc[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRoc[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRoc[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRoc[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRoc[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

## Combine graphs into single multiframe figure
## Function to pull out legend
extract_legend<-function(myggplot){
    tmp <- ggplot_gtable(ggplot_build(myggplot))
    leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
    legend <- tmp$grobs[[leg]]
    return(legend)
    }

# Dumbie graph
legend_plot <- ggplot(Filter_only_outRmd_data, aes(Avg_OC_Ratio, L_Hr_m2)) +
  geom_point(aes(color = Site)) +
  scale_color_manual(values = Site_colors) +
  theme_classic() 
# extract legend
QR_legend <- extract_legend(legend_plot)

# make multipframe figure
grid.arrange(arrangeGrob(qrplot_temp, qrplot_sal, qrplot_turb, qrplot_tpm, qrplot_oc, QR_legend, 
             nrow = 3,
             left = grid::textGrob(Hab_FR_Label, rot = 90, vjust = 1, 
                                          gp = gpar(fontsize = 12))))

#############################################
# Copy and Pasted from HCR code chunk. Not used up there, not sure if this is redundant to code above. Delete possibly?
############### Quantile Regresssion Analysis Values ################
#### HCR by Site with .5 quantile ###########
# select within each stie
SR_50quant <- Filter_only_outRmd_data %>%
  filter(Site == "San Rafael",
         L_Hr_m2 > FR_Site[[1,5]])
#hist(SR_50quant$L_Hr_m2, breaks = 20)

MB_50quant <- Filter_only_outRmd_data %>%
  filter(Site == "Morro Bay",
         L_Hr_m2 > FR_Site[[2,5]])
#hist(SR_50quant$L_Hr_m2, breaks = 20)

NPSM_50quant <- Filter_only_outRmd_data %>%
  filter(Site == "Shellmaker",
         L_Hr_m2 > FR_Site[[3,5]])
#hist(NPSM_50quant$L_Hr_m2, breaks = 20)

NPD_50quant <- Filter_only_outRmd_data %>%
  filter(Site == "Deanza",
         L_Hr_m2 > FR_Site[[4,5]])
#hist(NPD_50quant$L_Hr_m2, breaks = 20)

# combine each site's .5 quantile data together
Site_HCR_50quant <- rbind(SR_50quant, MB_50quant, NPSM_50quant, NPD_50quant)

HCR50_sum <- Site_HCR_50quant %>% 
  group_by(Site) %>% 
  summarize(avg_L_hr_m2 = mean(L_Hr_m2),
            SD = sd(L_Hr_m2),
            n_samples = length(L_Hr_m2),
            med = median(L_Hr_m2),
            var = var(L_Hr_m2))
# 443514/6689 --> largest residual / smallest residual - greater than 5 not good for ANOVA

HCR_50quant_box <- ggplot((Site_HCR_50quant), aes(Site, L_Hr_m2)) +
  geom_point(aes(fill = Site), shape = 21, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = Hab_FR_Label)

#plot(HCR_50quant_box)

####### Kruskal-Wallis test - tidy converts to clean dataframe
KW_HCR50_site <- broom::tidy(kruskal.test(L_Hr_m2 ~ Site, data = Site_HCR_50quant))


####### one-way ANOVA -- HCR by Site
HCR50_site_aov <- aov(L_Hr_m2 ~ Site, data = Site_HCR_50quant)
#summary(HCR_site_aov)
HCR50_aov_values <- broom::tidy(HCR50_site_aov)

########### Power analysis - sensitivity power analysis 
# one-way ANOVA effect size (n^2). n^2 = treatmentSumSquares / TotalSumSquares (TreatSS + ResidualSS) 
d_HCR50_site <- { HCR50_aov_values[[1,3]] / (HCR50_aov_values[[1,3]] + HCR50_aov_values[[2,3]]) }

# power analysis for "balanced one-way analysis of variance tests"
# For now Ted says to use the mean of the different sample sizes
HCR50_site_power <- pwr.anova.test(f = d_HCR50_site, 
                                 k = 4,
                                 n = mean(HCR50_sum$n_samples), 
                                 sig.level = 0.05,
                                 power = NULL) # solve for this
```

## Seston Quantity and Quality

```{r TPM_OC_text_values, include=FALSE}
TPM_OC_Values <- Master_analysis_table %>% 
  filter(Experiment == "Filtration") %>% 
  dplyr::select(Site, Avg_TPM_mg_L, Avg_OC_Ratio) %>% 
  na.omit()

SR_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "San Rafael")

MB_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "Morro Bay")

NPSM_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "Shellmaker")

NPD_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "Deanza")

NP_combo_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "Deanza" | Site == "Shellmaker")
```

Northern San Francisco Bay (San Rafael) TPM averaged `r round(mean(SR_TPM_OC$Avg_TPM_mg_L),2)` mg/L (*N* = `r nrow(SR_TPM_OC)`, *SD* = `r round(sd(SR_TPM_OC$Avg_TPM_mg_L),2)`), and Morro Bay TPM averaged `r round(mean(MB_TPM_OC$Avg_TPM_mg_L),2)` mg/L (*N* = `r nrow(MB_TPM_OC)`, *SD* = `r round(sd(MB_TPM_OC$Avg_TPM_mg_L),2)`) (Figure \ref{TPM_OC_model}).
Newport Bay (Deanza and Shellmaker) TPM averaged `r round(mean(NP_combo_TPM_OC$Avg_TPM_mg_L),2)` mg/L (*N* = `r nrow(NP_combo_TPM_OC)`, *SD* = `r round(sd(NP_combo_TPM_OC$Avg_TPM_mg_L),2)`).
Northern San Francisco Bay (San Rafael) OC averaged `r round(mean(SR_TPM_OC$Avg_OC_Ratio),2)` (*N* = `r nrow(SR_TPM_OC)`, *SD* = `r round(sd(SR_TPM_OC$Avg_OC_Ratio),2)`), and Morro Bay OC averaged `r round(mean(MB_TPM_OC$Avg_OC_Ratio),2)` (*N* = `r nrow(MB_TPM_OC)`, *SD* = `r round(sd(MB_TPM_OC$Avg_OC_Ratio,2))`).
Newport Bay (Deanza and Shellmaker) OC averaged `r round(mean(NP_combo_TPM_OC$Avg_OC_Ratio),3)` (*N* = `r nrow(NP_combo_TPM_OC)`, *SD* = `r round(sd(NP_combo_TPM_OC$Avg_OC_Ratio),2)`) (Figure \ref{TPM_OC_model}).

## Filter Feeding Community

```{r Bivalve_Den_Setup, include=FALSE}
################### Bivalve Density by Site #########################
# read in bivalve density data from cleaned data file
Bivalve_Den_data <- fread(file.path(Bivalve_output_directory, "Bivalve_Density_summary.csv"))

# replace NAs with 0s
Bivalve_Den_data[is.na(Bivalve_Den_data)] <- 0

Bivalve_Den_data <- Bivalve_Den_data %>% 
  # move species from individual columns to a single column with new density column to contain data
  pivot_longer(c(Adula_diegensis_m2, Musculista_senhousia_m2, Mytilus_galloprovincialis_m2,
                 Ostrea_lurida_m2, Crassostrea_gigas_m2, Argopecten_ventricosa_m2, 
                 Geukensia_demissa_m2, Crassostrea_gigas_m2), 
               names_to = "Species", 
               values_to =  "individuals_m2") %>% 
  # remove "_m2" from species names in Species column
  mutate(Species = str_replace(Species, "_m2", ""),
         Species = str_replace(Species, "_", " "))

#Reorder levels of Site factor - display North to South
order_by_site <- c("San Rafael", "Morro Bay", "Shellmaker", "Deanza") 
Bivalve_Den_data <- arrange(transform(Bivalve_Den_data, Site = factor(Site,levels = order_by_site)), Site)
# Density values for in text
SR_Den <- Bivalve_Den_data %>% 
  filter(Site == "San Rafael",
         individuals_m2 > 0) %>% 
  summarise(total_den = sum(individuals_m2))

MB_Den <- Bivalve_Den_data %>% 
  filter(Site == "Morro Bay",
         individuals_m2 > 0) %>% 
  summarise(total_den = sum(individuals_m2))

NPSM_Den <- Bivalve_Den_data %>% 
  filter(Site == "Shellmaker",
         individuals_m2 > 0) %>% 
  summarise(total_den = sum(individuals_m2))
  
NPD_Den <- Bivalve_Den_data %>% 
  filter(Site == "Deanza",
         individuals_m2 > 0) %>% 
  summarise(total_den = sum(individuals_m2))
## Species list
NPSM_species <- Bivalve_Den_data %>% 
  filter(Site == "Shellmaker",
         individuals_m2 > 0) %>% 
  group_by(Species) %>% 
  summarise(species_den = individuals_m2)
 
NPD_species <- Bivalve_Den_data %>% 
  filter(Site == "Deanza",
         individuals_m2 > 0) %>% 
  group_by(Species) %>% 
  summarise(species_den = individuals_m2)
  
```

In November 2017 the estimated bivalve density at San Rafael was `r SR_Den` individuals/m^2^, all of which were *Ostrea lurida* (Figure \ref{Bivalve_Den}).
Other bivalves were noted, but were rare, and were not detected in sample bags (C. Zabin, unpublished data). 
Morro Bay had an estimated `r MB_Den` *Crassostrea gigas* individuals/m^2^ in the summer of 2018 (Morro Bay Oyster Company); personal field observations confirm the lack of bivalve fouling on the aquaculture lines.
In May 2018, Shellmaker had an estimated `r NPSM_Den` individuals/m^2^, composed of *Adula diegensis* (`r NPSM_species[NPSM_species$Species=="Adula diegensis",2]` individuals/m^2^), *Musculista senhousia* (`r NPSM_species[NPSM_species$Species=="Musculista senhousia",2]` individuals/m^2^), *O. lurida* (`r NPSM_species[NPSM_species$Species=="Ostrea lurida",2]` individuals/m^2^), *Mytilus galloprovincialis* (`r NPSM_species[NPSM_species$Species=="Mytilus galloprovincialis",2]` individuals/m^2^), *Geukensia demissa* (`r NPSM_species[NPSM_species$Species=="Geukensia demissa",2]` individuals/m^2^), and *Argopecten ventricosa* (`r NPSM_species[NPSM_species$Species=="Argopecten ventricosa",2]` individuals/m^2^) (Figure \ref{Bivalve_Den}).
Deanza had an estimated `r NPD_Den` individuals/m^2^ in May 2018, and was composed of *M. senhousia* (`r NPD_species[NPD_species$Species=="Musculista senhousia",2]` individuals/m^2^), *A. diegensis* (`r NPD_species[NPD_species$Species=="Adula diegensis",2]` individuals/m^2^), *O. lurida* (`r NPD_species[NPD_species$Species=="Ostrea lurida",2]` individuals/m^2^), *M. galloprovincialis* (`r NPD_species[NPD_species$Species=="Mytilus galloprovincialis",2]` individuals/m^2^) (Figure \ref{Bivalve_Den}).

```{r Bivalve_Den_bargraph, echo=FALSE, fig.align="center", fig.asp=0.618, fig.width=6, warning= F, message= F, fig.keep='all', dev='pdf', out.width="70%", fig.cap="Bivalve community density and composition for each study site. San Rafael data were collected November 2017 by the Zabin lab at SERC, Morro Bay was estimated by Morro Bay Oyster Company in 2018, and Shellmaker and Deanza were surveyed in May 2018 by the Zacherl lab at CSUF.  \\label{Bivalve_Den}"}

#unique(Bivalve_Den_data$Species) # list each individual bivalve species
# Assign species to specific color
Species_colors <- c("Adula diegensis" = "#8D8680",
                    "Argopecten ventricosa" = "#CCC3C5",
                    "Crassostrea gigas" = "#6D6340",
                    "Geukensia demissa" = "#524D4F",
                    "Musculista senhousia" = "#B0915B",
                    "Mytilus galloprovincialis" = "#7E4B41",
                    "Ostrea lurida" = "#9986A5",
                    "Unknown mussel" = "black")

# Stacked bar graph
Bivalve_Den_graph <- ggplot(Bivalve_Den_data, aes(Site)) + 
  geom_col(aes(x = Site, y = individuals_m2, fill = Species)) +
 # geom_text(aes(label = round(Bivalve_den_m2,0), y = Bivalve_den_m2), 
          #  vjust = -0.5) +
  theme_classic() +
  theme(legend.text = element_text(face = "italic")) +
  scale_fill_manual(values = Species_colors) +
  scale_y_continuous(breaks = seq(0, max(Bivalve_Den_data$Bivalve_den_m2), by = 500)) +
  labs(y = expression("Individuals m"^-2))
Bivalve_Den_graph
```



```{r Bivalve_biomass_setup, include=FALSE}
######### Biomass by Site ############################
# excavation quadrat area m^2 (0.25 x 0.25m)
NP_excavation_quad_area = 0.0625

### vvv incorrect calculation vvv - mean of quadrat DTW instead of total
# calculate DTW / m^2 / species 
#NPD_biv_DTW_m2 <- NPD_bivalve_outrmd %>% 
 # group_by(excavation_sample, species) %>% 
  #summarize(mean_DTW = mean(tissue_dry_weight_g)) %>% # show the average DTW for each species within each excav sample
  #ungroup() %>% # undo previous grouping by excavation_sample and species
  #group_by(species) %>% # new grouping by species to determine the average DTW by species across all quads
  #summarize(mean_DTW_m2 = mean(mean_DTW/NP_excavation_quad_area)) %>% 
  #mutate(Site = "Deanza") %>%  # add new column with site info for bar graph
  #rename(Species = species) # rename column with capital case
 ###### ^^^^^ end of incorrect code ^^^ 

#### Check above code for calculations

# sum the total DTW (g) within each excavation sample by species
a <- NPD_bivalve_outrmd %>% 
  group_by(excavation_sample, species) %>% 
  summarize(total_q_dtw = sum(tissue_dry_weight_g)) %>% 
  ungroup() 
# fill in implicit zeros(NA) to dataframe
a2 <- a %>% 
  complete(excavation_sample, species) %>% 
  replace_na(list(total_q_dtw = 0))
# take mean DTW across quadrats
b <- a2 %>% 
  group_by(species) %>% 
  summarize(dtw_mean_q = mean(total_q_dtw)) %>% 
  ungroup()
# adjust per m^2
NPD_biv_DTW_m2 <- b %>% 
  mutate(mean_DTW_g_m2 = dtw_mean_q/NP_excavation_quad_area) %>% 
  mutate(Site = "Deanza") %>%  # add new column with site info for bar graph
  rename(Species = species) %>% # rename column with capital case
  dplyr::select(Site, Species, mean_DTW_g_m2)

###### Values to populated text
NPD_total_DTW <- NPD_biv_DTW_m2 %>% 
  summarise(total = sum(mean_DTW_g_m2))

# Keep object names for text reporting
NPD_species_DTW <- NPD_biv_DTW_m2 %>% 
  dplyr::select(Species, mean_DTW_g_m2)

```


Direct biomass data were only available for Deanza, which estimated `r round(NPD_total_DTW, 2)` g/m^2^ of bivalve dry tissue weight (DTW) (Figure \ref{Bivalve_DTW_Biomass}). 
*O. lurida* had the highest DTW with `r round(NPD_species_DTW[NPD_species_DTW$Species=="Ostrea lurida",2],2)` g/m^2^, followed by *M. galloprovincialis* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Mytilus galloprovincialis",2],)` g/m^2^), an unknown *Modiolus* sp. (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Unknown mussel",2],2)` g/m^2^), *A. diegensis* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Adula diegensis",2],2)` g/m^2^), and *M. senhousia* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Musculista senhousia",2],2)` g/m^2^) (Figure \ref{Bivalve_DTW_Biomass}).

*O. lurida* had the highest DTW with `r round(NPD_species_DTW[NPD_species_DTW$Species=="Ostrea lurida",2],2)` g/m^2^, followed by *M. galloprovincialis* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Mytilus galloprovincialis",2],)` g/m^2^), an unknown *Modiolus* sp. (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Unknown mussel",2],2)` g/m^2^), *A. diegensis* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Adula diegensis",2],2)` g/m^2^), and *M. senhousia* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Musculista senhousia",2],2)` g/m^2^) (Figure \ref{Bivalve_DTW_Biomass}).

```{r Bivalve_biomass_bargraph, echo=FALSE, fig.align="center", fig.asp=0.618, fig.width=6, warning= F, message= F, fig.keep='all', dev='pdf', out.width="70%", fig.cap="Estimated biomass of \\textit{O. lurida} habitat at Deanza and San Rafael sites. Data for Deanza was collected by the Zacherl lab (CSUF) in May 2018, the relationship between \\textit{O. lurida} shell length and dry tissue weight (DTW) from the survey was used to estimate DTW at San Rafael with shell length data collected by the Zabin lab (SERC) in November 2017. \\label{Bivalve_DTW_Biomass}"}

####### Bar graph NPD Biomass / DTW by species
Bivalve_Biomass_graph <- ggplot(NPD_biv_DTW_m2, aes(Site)) + 
  geom_col(aes(x = Site, y = mean_DTW_g_m2, fill = Species)) +
  theme_classic() +
  scale_fill_manual(values = Species_colors) +
  theme(legend.text = element_text(face = "italic")) +
  #scale_y_continuous(breaks = seq(0, max(NPD_Posbiomass_outlier$tissue_dry_weight_g), by = 500)) +
  labs(y = expression("Dry Tissue Weight (g m" ^-2* ")"))
## Add total Biomass on top of bars
Bivalve_Biomass_graph
```

```{r Estimate_filtr_w_other_studies}
##### Compare previous filtration studies to our HCR findings

## Gray & Langdon (2018) ####
# O. lurida clearance rate mean L/hr/g (DTW) Yaquina Bay, OR
gray_dry_cr_oly <- 0.78 # Dry season
gray_wet_cr_oly <- 0.19 # Wet season

# O. lurida all season model insitu - as selected by stepwise regression analysis of temp, salinity, total particulate matter, organic content. Per 1 g of biomass DTW
estimate_oly_cr_gray <- function (temp, OC, biomass_g){
  clearance = (-0.95 + 2.49*OC + 0.06*temp)*biomass_g
  return(clearance)
  }

## Deanza mean O.lurida grams biomass per m^2 of habitat
npd_dtw_oly <- as.numeric(NPD_species_DTW[NPD_species_DTW$Species=="Ostrea lurida","mean_DTW_g_m2"])

# Mean Seston Organic Content - Deanza
npd_OC <- mean(NPD_TPM_OC$Avg_OC_Ratio)

# Mean Temperature - Deanza
npd_temp <- mean(Filter_only_outRmd_data[Filter_only_outRmd_data[,Site == "Deanza"], "Temp_C_Up"]$Temp_C_Up)

# estimate clearance rate with NPD biomass & Gray's model L/hr/g/m^2
NPD_oly_cr_est <- estimate_oly_cr_gray(npd_temp, npd_OC, npd_dtw_oly) / npd_dtw_oly

#### 
# M. gigas clearance rate mean L/hr/g (DTW)
gray_dry_cr_gigas <- 0.95
gray_wet_cr_gigas <- 1.06

## Wheat & Ruesink (2013) ####
# M gigas clearance rate L/hr/g grown on long-lines in Willipa Bay, WA
wheat_cr_gigas <- 0.73


```

