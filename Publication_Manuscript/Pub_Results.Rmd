---
bibliography: library.bib
csl: inter-research-science-center.csl
header-includes: 
- \usepackage{setspace}
- \doublespacing
- \usepackage{rotating}
- \usepackage{booktabs}
- \usepackage{makecell}
- \usepackage{gensymb}
- \usepackage{upgreek}
- \usepackage{pdfpages}
- \usepackage{caption}
output: 
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
    keep_tex: yes
indent: true
---
```{r setup, include=FALSE}

library(knitr)
# Troubleshooting knitting to pdf
#options(tinytex.verbose = TRUE)

# set default settings to save figures in Figures folder
knitr::opts_knit$set(root.dir = normalizePath('../'), # set working directory to project level
                     fig.path = "./Pub_Manuscript/Figures")

# Set chunk options- no warning messages, and place figures near where they are referenced.             
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.width = 6,
                      fig.asp=0.618, 
                      fig.align = "center")

```

```{r load_packages_data, include=FALSE}
################### Packages ###########################
library(dplyr)
library(tidyr)
library(readr) # reading in files 
library(magrittr) # pipe function %>%
library(data.table) # file read function and needed for xtable
library(lubridate) # dealing with time and dates
library(ggplot2) # graphing
library(ggthemes) # theme_classic() for graphs
library(stringr) # dealing with text strings
library(hms) # dealing with time
library(viridisLite) # color palette
library(forcats) # dealing with factors
library(gridExtra)
library(ggpmisc) # package for adding line equation and R^2 to plots
library(viridis) # Color palette - Options: magma , inferno, plasma, viridis, cividis
library(scales) # add different number scales in graphs
library(cowplot) # for arranging multiple plots into a single figure
library(wesanderson) # color palette
library(olsrr) # regression output table
library(xtable) # creates tables in LaTex
library(MASS) # collection of datasets and functions
library(ISLR) # Dataset in Intro to statistical learning
library(knitr)
library(tinytex) # LaTeX conversion package
library(broom) # tidy() function to convert t.test to dataframe
library(agricolae) # post-hoc Tukey test
library(multcompView) #
library(quantreg) # Quantile regression
library(grid) # makes test grobs useful for multiframe figures
library(pwr) # Power analysis
library(data.table)

# source functions 
source("./functions.R")
# define output directory
output_directory = "./output"
# create output directories (same as Insitu_Filtraiton_Analysis_script.Rmd)
createOutputDirectories()

##################### Read in Data from  project output ##########################
All_data <- fread(file.path('./output/4_Filtration_Calculations/Filtration_Summary', "Master_Filtration_analysis_table.csv"))
Avg_TPM_summary_table <- fread(file.path('./output/5_TPM_OC_Summary', "Avg_TPM_summary_table.csv"))
Density_directory = "./Data/bivalve_density_community"
NPD_bivalves = fread(file.path(Density_directory, "Insitu_Filter_NPD_bivalve_biomass_data.csv"))
Sbs_correction_data = fread(file.path('./output/6_Sbs_Corrections_Summary', "sbs_correction_summary.csv"))


##################### Data #########################################
# Remove San Diego filtration trial - not applicable to this analysis - different experimental set up
Master_analysis_table <- All_data %>% 
  filter(!Site %in% c("San Diego"))

# Order Sites by Latitude
site_by_lat <- c("San Rafael", "Morro Bay", "Shellmaker", "Deanza") 
site_by_lat_abv <- c("SR", "MB", "NPSM", "NPD")

# transform Site variable to factor and arange by Latitude
Master_analysis_table <- arrange(transform(Master_analysis_table, Site = factor(Site,levels = site_by_lat)), Site)

# All Trials, NPD 2019_4_17 removed - below Chl detection limit
site_data_outrmd <- Master_analysis_table %>% 
  filter(round(Chl_ug_L_Up,2) > 0.1 | round(Chl_ug_L_Up,2) < -0.1)

 # Filtration trials only - exclude Negative controls
Filtration_only_data <- Master_analysis_table %>% 
  filter(Experiment %in% c("Filtration"))

# Negative Control Trials only 
Control_only_data <- Master_analysis_table %>% 
  filter(grepl("Neg*", Master_analysis_table$Experiment))

# Filtration only - remove trials with Chl a within sensor detection limit (+- 0.1ug/L)
Filter_only_outRmd_data <- Filtration_only_data %>% 
  filter(round(Chl_ug_L_Up,2) > 0.1 | round(Chl_ug_L_Up,2) < -0.1)

############### Values for populating Results in text #####
# SR filtration trial data
SR_filter_sum <- Filter_only_outRmd_data %>% 
  filter(Site %in% c("San Rafael"),
         Experiment %in% c("Filtration")) 

MB_filter_sum <- Filter_only_outRmd_data %>% 
  filter(Site %in% c("Morro Bay"),
         Experiment %in% c("Filtration"))

NPD_filter_sum <- Filter_only_outRmd_data %>% 
  filter(Site %in% c("Deanza"),
         Experiment %in% c("Filtration"))

NPSM_filter_sum <- Filter_only_outRmd_data %>% 
  filter(Site %in% c("Shellmaker"),
         Experiment %in% c("Filtration"))

# Single out NPD 2019_4_17 for stats reporting
NPD_2019_4_17 <- read_csv("output/4_Filtration_Calculations/Insitu_Filter_NPD_2019_4_17.csv")
NPD_2019_4_17 %<>% filter(Experiment == "Filtration") 

# Deanza only - Filtration trials only
Deanza_Filter_data <- Master_analysis_table %>% 
  filter(Site %in% c("Deanza"),
         Experiment %in% c("Filtration"))
# Deanza only- Positive Filtration only
Deanza_PosFilter_data <- Deanza_Filter_data %>% 
  filter(pcnt_Chl_rmvd > 0)

### Average TPM Data ######################
Avg_TPM_summary_table <- arrange(transform(Avg_TPM_summary_table, Site = factor(Site, levels = site_by_lat)), Site)

### Reporting Restoration vs aquaculture in Abstract
abstract_num <- Filter_only_outRmd_data %>% 
  mutate(habitat = ifelse(Site %in% c("San Rafael", "Deanza", "Shellmaker"), "Rest", "Aqcltr")) %>%
  dplyr::select(Site, L_hr_m2, habitat) %>% 
  group_by(habitat) %>% 
  summarise(mean_HCR = mean(L_hr_m2),
            SD = sd(L_hr_m2),
            st_error = SD / sqrt(length(L_hr_m2)))

###################### Functions ####################################

# function to calculate average, standard deviation, outlier data. 
find_outlier <- function(data){
  average <- mean(data)
  stdev <- sd(data)
  # Outlier data defined by outside 2.2 standard deviations from the mean 
  outlier <- ifelse((data < (average - 2.2 * stdev) | (data > (average + 2.2 * stdev))),
         TRUE, FALSE)
}
```

```{r, Graphing_colors_labels, include=FALSE}

############### Site Color Assignments ###############
viridis(4)
Site_colors <- c("San Rafael" = "#440154FF",
                 "Morro Bay" = "#31688EFF",
                 "Shellmaker" = "#35B779FF",
                 "Deanza" = "#FDE725FF")

############### Bivalve Species Color Assignments ###############
# wes_palettes # lists all current palattes 
# Color palette for Species - Royal2, Darjeeling2, Cavalcanti1, BottleRocket2, IsleofDogs1
density_palette <- rev(wes_palette("IsleofDogs1", 7, type = c("continuous")))
Species_colors <- c("Adula diegensis" = "#8D8680",
                    "Argopecten ventricosa" = "#CCC3C5",
                    "Crassostrea gigas" = "#6D6340",
                    "Geukensia demissa" = "#524D4F",
                    "Musculista senhousia" = "#B0915B",
                    "Mytilus galloprovincialis" = "#7E4B41",
                    "Ostrea lurida" = "#9986A5",
                    "Unknown mussel" = "black")

################# Labels for graphs  #########################
Hab_FR_Label <- expression('Habitat Clearance Rate (L hr'^-1*'m'^-2*')')
Prt_Chl_Label <- expression(paste('Percent Chlorophyll ', alpha, ' Removed'))
Chl_ugL_Label <- expression(paste("Chlorophyll ", alpha, " (", mu, "g/L) "))
TPM_Label <- c('Total Particulate Matter mg/L')
PIM_Label <- c('Particulate Inorganic Matter mg/L')


```

```{r Site WQ ANOVA, fig.width=7, fig.asp=1.2, fig.cap="Box plots of ambient (upstream) A) temperature, B) salinity, C) turbidity, D) total particulate matter, E) organic content, and F) chlorophyll $\\alpha$ from filtration trials. One-way ANOVAs compared the difference between water quality variables and site. Significantly different results were grouped by a post-hoc Tukey's HSD; significantly different sites do not share a common letter, and non-significant differences share letters. Site effects on OC were significant, and a Newman-Keuls post-hoc analysis determined a significant difference between San Rafael and Deanza undetected by Tukey's HSD. Trials were conducted from February 2018 to June 2019 at San Rafael, CA (restored reefs); Morro Bay, CA (Morro Bay Oyster Company, aquaculture); and Newport Bay, CA (Shellmaker and Deanza, restored beds). \\label{Site_WQ_boxplot}" }

#########################
###### Temperature 
#hist(Filter_only_outRmd_data$Temp_C_Up)
site_temp_aov <- aov(Temp_C_Up ~ Site, data = Filter_only_outRmd_data)
#summary(site_temp_aov)
temp_aov_values <- broom::tidy(site_temp_aov) # put ANOVA model into tibble to extract individual values
# Post-Hoc Tukey test
tukey_temp <- TukeyHSD(site_temp_aov)
#tukey_temp
#plot(tukey_temp)

#Honestly Significant Difference test --> Don't need if non-significant ANOVA
hds_temp <- HSD.test(site_temp_aov, trt = "Site")
#hds_temp

Temp_box <- ggplot(Filter_only_outRmd_data, aes(Site, Temp_C_Up)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = expression(paste("Temperature (",degree, "C)")),
       title = "A)")

#########################
####### Salinity
#hist(site_data_outrmd$Sal_ppt_Up)
site_sal_aov <- aov(Sal_ppt_Up ~ Site, data = Filter_only_outRmd_data)
sal_aov_values <- broom::tidy(site_sal_aov)

# Post-Hoc Tukey test
tukey_sal <- TukeyHSD(site_sal_aov)
#tukey_sal
#plot(tukey_sal)

#Honestly Significant Difference test
hds_sal <- HSD.test(site_sal_aov, trt = "Site")
#hds_sal

Sal_box <- ggplot(Filter_only_outRmd_data, aes(Site, Sal_ppt_Up)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = "Salinity (ppt)",
       title = "B)") +
  annotate("text", "San Rafael", 
           max((site_data_outrmd %>% 
                  filter(Site == "San Rafael"))$Sal_ppt_Up) + 0.75, label = "a") +
  annotate("text", "Morro Bay", 
           max((site_data_outrmd %>% 
                  filter(Site == "Morro Bay"))$Sal_ppt_Up) + 0.75, label = "b") +
  annotate("text", "Shellmaker", 
           max((site_data_outrmd %>% 
                  filter(Site == "Shellmaker"))$Sal_ppt_Up) + 0.75, label = "b") +
  annotate("text", "Deanza", 
           max((site_data_outrmd %>% 
                  filter(Site == "Deanza"))$Sal_ppt_Up) + 0.75, label = "b") 

###########################
######### Turbidity
#hist(site_data_outrmd$Turbidity_NTU_Up)
site_turb_aov <- aov(Turbidity_NTU_Up ~ Site, data = Filter_only_outRmd_data)
turb_aov_values <- broom::tidy(site_turb_aov)

# Post-Hoc Tukey test
tukey_turb <- TukeyHSD(site_turb_aov)
#tukey_turb
#plot(tukey_turb)

#Honestly Significant Difference test
hds_turb <- HSD.test(site_turb_aov, trt = "Site")
#hds_turb

Turb_box <- ggplot(Filter_only_outRmd_data, aes(Site, Turbidity_NTU_Up)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = "Turbidity (NTU)",
       title = "C)") +
  annotate("text", "San Rafael", 
           max((site_data_outrmd %>% 
                  filter(Site == "San Rafael"))$Turbidity_NTU_Up) + 2, 
           label = "a") +
  annotate("text", "Morro Bay", 
           max((site_data_outrmd %>% 
                  filter(Site == "Morro Bay"))$Turbidity_NTU_Up) + 2, 
           label = "b") +
  annotate("text", "Shellmaker", 
           max((site_data_outrmd %>% 
                  filter(Site == "Shellmaker"))$Turbidity_NTU_Up) + 2, 
           label = "c") +
  annotate("text", "Deanza", 
           max((site_data_outrmd %>% 
                  filter(Site == "Deanza"))$Turbidity_NTU_Up) + 2, 
           label = "c") 

########################
####### TPM
#hist(site_data_outrmd$Avg_TPM_mg_L)
site_TPM_aov <- aov(Avg_TPM_mg_L ~ Site, data = Filter_only_outRmd_data)
TPM_aov_values <- broom::tidy(site_TPM_aov)

# Post-Hoc Tukey test
tukey_tpm <- TukeyHSD(site_TPM_aov)
#tukey_tpm
#plot(tukey_tpm)

#Honestly Significant Difference test
hds_tpm <- HSD.test(site_TPM_aov, trt = "Site")
#hds_tpm

Tpm_box <- ggplot(Filter_only_outRmd_data, aes(Site, Avg_TPM_mg_L)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = expression(paste("Total Particulate Matter (mg/L)")),
       title = "D)") +
  annotate("text", "San Rafael", 
           max((site_data_outrmd %>% 
                  filter(Site == "San Rafael"))$Avg_TPM_mg_L) + 10, label = "a") +
  annotate("text", "Morro Bay", 
           max((site_data_outrmd %>% 
                  na.omit() %>% 
                  filter(Site == "Morro Bay"))$Avg_TPM_mg_L) + 10, label = "b") +
  annotate("text", "Shellmaker", 
           max((site_data_outrmd %>% 
                  filter(Site == "Shellmaker"))$Avg_TPM_mg_L) + 10, label = "b") +
  annotate("text", "Deanza", 
           max((site_data_outrmd %>% 
                  na.omit() %>% 
                  filter(Site == "Deanza"))$Avg_TPM_mg_L) + 10, label = "b") 

#####################
####### OC
#hist(site_data_outrmd$Avg_OC_Ratio)
site_OC_aov <- aov(Avg_OC_Ratio ~ Site, data = Filter_only_outRmd_data %>% na.omit())
OC_aov_values <- broom::tidy(site_OC_aov)

# Post-Hoc Tukey test
tukey_oc <- TukeyHSD(site_OC_aov)

#Honestly Significant Difference test
hds_oc <- HSD.test(site_OC_aov, trt = "Site")
# Not not showing differences between sites

# Less conservative Post-hoc analysis - Newman-keuls (also good for simple group structure)
library(DescTools)
NK_oc <- PostHocTest(site_OC_aov, method = "newmankeuls", conf.level = 0.95)

OC_box <- ggplot(Filter_only_outRmd_data, aes(Site, Avg_OC_Ratio)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = "Organic Content (Ratio)",
       title = "E)")

#########################
###### Chlorphyll
#hist(site_data_outrmd$Chl_ug_L_Up)
site_chl_aov <- aov(Chl_ug_L_Up ~ Site, data = Filter_only_outRmd_data)
#summary(site_chl_aov)
chl_aov_values <- broom::tidy(site_chl_aov) # put ANOVA model into tibble to extract individual values
# Post-Hoc Tukey test
tukey_chl <- TukeyHSD(site_chl_aov, ordered = FALSE, conf.level = 0.95)
#tukey_chl
#plot(tukey_chl)

#Honestly Significant Difference test
hds_chl <- HSD.test(site_chl_aov, trt = "Site")
#hds_chl

Chl_box <- ggplot(Filter_only_outRmd_data, aes(Site, Chl_ug_L_Up)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = expression(paste("Chlorophyll ",alpha, " (", mu,"g/L)")),
       title = "F)") 

grid.arrange(Temp_box, Sal_box, Turb_box, Tpm_box, OC_box, Chl_box,nrow=3)
```

# Results

Twenty-five experimental trials, across the four study sites, were included in the analyses; 21 filtration trials and four control trials.
A single filtration trial at Deanza (2019-4-17; Table 1) was removed from the analysis because the mean upstream Chl $\alpha$ (*M* = `r round(mean(NPD_2019_4_17$Chl_ug_L_Up),1)`, *SD* = `r round(sd(NPD_2019_4_17$Chl_ug_L_Up),2)`) was within the detection limit of the sensor (± 0.1 $\mu$g/L). 
Filtration trials across sites were not distributed equally (Table 1), Deanza had more than twice the amount filtration trials (*N* = 9) as San Rafael (*N* = 4), Morro Bay (*N* = 4), and Shellmaker (*N* = 4), while each site had a single control.

Ambient water quality during filtration trials varied within and among sites (Figure \ref{Site_WQ_boxplot}). 
Salinity was significantly different among sites as determined by a one-way ANOVA at a *p* < 0.05 (*F*(`r sal_aov_values[[1,2]]`, `r sal_aov_values[[2,2]]`) = `r round(sal_aov_values[[1,5]],2)`, *p* < 0.001), along with turbidity (*F*(`r turb_aov_values[[1,2]]`, `r turb_aov_values[[2,2]]`) = `r round(turb_aov_values[[1,5]],2)`, *p* < 0.001), and TPM (*F*(`r TPM_aov_values[[1,2]]`, `r TPM_aov_values[[2,2]]`) = `r round(TPM_aov_values[[1,5]],2)`, *p* = < 0.001) (Figure \ref{Site_WQ_boxplot}).  
Temperature (*F*(`r temp_aov_values[[1,2]]`, `r temp_aov_values[[2,2]]`) = `r round(temp_aov_values[[1,5]],2)`, *p* = `r format(round(temp_aov_values[[1,6]],2), nsmall = 2)`), and Chl $\alpha$ (*F*(`r chl_aov_values[[1,2]]`, `r chl_aov_values[[2,2]]`) = `r round(chl_aov_values[[1,5]],2)`, *p* = `r round(chl_aov_values[[1,6]],2)`) were not different among sites (Figure \ref{Site_WQ_boxplot}).
OC was significant among sites (*F*(`r OC_aov_values[[1,2]]`, `r OC_aov_values[[2,2]]`) = `r round(OC_aov_values[[1,5]],2)`, *p* = `r round(OC_aov_values[[1,6]],2)`), but the post-hoc Tukey HSD did not reveal significant differences among sites. 
Therefore, I use a less conservative post-hoc analysis, the Newman-Keuls method, and found that OC was significantly different between Shellmaker and Deanza (*p* = `r round(NK_oc$Site[[3,4]],2)`).

```{r ChlRmd_Site_boxplot, echo=FALSE, fig.keep = 'all', dev = 'pdf', fig.align= "center", fig.asp=0.618, out.width="70%", message= F, fig.cap="Box plots of percent chlorophyll $\\alpha$ removal (Chl\\textsubscript{up} - Chl\\textsubscript{down} / Chl\\textsubscript{up} * 100) during filtration trials, control trials are listed in Table 1. Each data point is the mean of a single filtration trial. Filtration trials were conducted between February 2018 to June 2019 at San Rafael, CA (restored reefs); Morro Bay, CA (Morro Bay Oyster Company aquaculture); and Newport Bay, CA (Shellmaker and Deanza, restored beds). \\label{ChlRmd_Site_boxplot}"}

### Chl removed summary - filtration trials only
Chlrmd_Site <- Filter_only_outRmd_data %>% 
  group_by(Site) %>% 
  summarize(avg_Chlrmd = mean(pcnt_Chl_rmvd),
            SD = sd(pcnt_Chl_rmvd),
            n_samples = length(pcnt_Chl_rmvd))

Chlrmd_site_aov <- aov(pcnt_Chl_rmvd ~ Site, data = Filter_only_outRmd_data)
#summary(HCR_site_aov)
Chlrmd_aov_values <- broom::tidy(Chlrmd_site_aov)

# Post-Hoc Tukey test
tukey_Chlrmd <- TukeyHSD(Chlrmd_site_aov)
#tukey_Chlrmd
#plot(tukey_Chlrmd)
#Honestly Significant Difference test
hds_Chlrmd <- HSD.test(Chlrmd_site_aov, trt = "Site")
#hds_Chlrmd

#### exteme Shellmaker may violate ANOVA equal variance assumption
## more suited for Kruskal-Wallis
KW_Chlrmd_site <- broom::tidy(kruskal.test(pcnt_Chl_rmvd ~ Site, data = Filter_only_outRmd_data))


########### Power analysis - sensitivity power analysis 
# one-way ANOVA effect size (n^2). n^2 = treatmentSumSquares / TotalSumSquares (TreatSS + ResidualSS) 
d_Chlrmd_site <- {Chlrmd_aov_values[[1,3]] / (Chlrmd_aov_values[[1,3]] + Chlrmd_aov_values[[2,3]]) }

# power analysis for "balanced one-way analysis of variance tests"
# For now Ted says to use the mean of the different sample sizes
Chlrmd_site_power <- pwr.anova.test(f = d_Chlrmd_site, 
                                 k = 4,
                                 n = mean(Chlrmd_Site$n_samples), 
                                 sig.level = 0.05,
                                 power = NULL) # solve for this

Chl_Rmd_box <- ggplot(Filter_only_outRmd_data, aes(Site, pcnt_Chl_rmvd)) +
  geom_boxplot(color = Site_colors) + 
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.5, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = Prt_Chl_Label)

plot(Chl_Rmd_box)

```

## Percent Chlorophyll $\alpha$ Removal

The mean percent Chl $\alpha$ removal at the San Rafael site was `r round(mean(SR_filter_sum$pcnt_Chl_rmvd),1)`\% (*N* = `r nrow(SR_filter_sum)`, *SD* = `r round(sd(SR_filter_sum$pcnt_Chl_rmvd),2)`) (Figure \ref{ChlRmd_Site_boxplot}) and was `r round(SR_NC_sum$pcnt_Chl_rmvd,1)`\% in the single control trial (Table 1). 
Filtration trials at Morro Bay had a mean Chl $\alpha$ removal of `r round(mean(MB_filter_sum$pcnt_Chl_rmvd),1)`\% (*N* = `r nrow(MB_filter_sum)`, *SD* = `r round(sd(MB_filter_sum$pcnt_Chl_rmvd),1)`) and `r round(MB_NC_sum$pcnt_Chl_rmvd,1)`\% during the control trial. 
At Deanza, mean Chl $\alpha$ removal was `r round(mean(NPD_filter_sum$pcnt_Chl_rmvd),1)`\% (*N* = `r nrow(NPD_filter_sum)`, *SD* =  `r round(sd(NPD_filter_sum$pcnt_Chl_rmvd),1)`) and `r round(mean(NPD_NC_sum$pcnt_Chl_rmvd))`\% Chl $\alpha$ removal during the control trial.
Mean Shellmaker Chl $\alpha$ removal was `r round(mean(NPSM_filter_sum$pcnt_Chl_rmvd),1)` \% (*N* = `r nrow(NPSM_filter_sum)`, *SD* = `r round(sd(NPSM_filter_sum$pcnt_Chl_rmvd),1)`) (Figure \ref{ChlRmd_Site_boxplot}), and its control trial was `r round(mean(NPSM_NC_sum$pcnt_Chl_rmvd),1)` \% (Table 1).
Chl $\alpha$ removal in filtration trials did not differ significantly between sites (one-way Kruskal-Wallis, *p* = `r round(KW_Chlrmd_site$p.value,2)`).


```{r Random_Forest_DrNichols, include=FALSE}
##################### Estimate missing values (Imputation) ##########################
#In our data we are missing 4 out of 400ish values.  (cells)
#Because we have limited observations but a high number of variables simply
#removing incomplete observations is too costly

#For imputation we are doing a non parametric random forest imputation.  See missForest documentation for details/references.  
#Stekhoven, D.J. and Buehlmann, P. (2012), "missForest - nonparametric missing value imputation for mixed-type data', Bioinformatics, 28(1) 2012, 112-118, doi: 10.1093/bioinformatics/btr597


# Import data
Analysis_data <- Master_analysis_table %>% 
    mutate(Date = mdy(Date),
         L_hr_m2 = L_hr_m2,
         pcnt_Chl_rmvd = pcnt_Chl_rmvd,
         Temp_C_Up = Temp_C_Up,
         Sal_ppt_Up = Sal_ppt_Up,
         Turbidity_NTU_Up = Turbidity_NTU_Up,
         Avg_TPM_mg_L = Avg_TPM_mg_L,
         Avg_OC_Ratio = Avg_OC_Ratio,
         Chl_ug_L_Up = Chl_ug_L_Up,
         Chl_ug_L_Down = Chl_ug_L_Down,
         avg_depth_m = avg_depth_cm/100,
         d_bw_sondes_m = d_bw_sondes_m,
         avg_m_sec = avg_m_hr/3600) %>%
  dplyr::select(Site, Date, Experiment, Temp_C_Up, Sal_ppt_Up, Turbidity_NTU_Up, Avg_TPM_mg_L, 
          Avg_OC_Ratio, Chl_ug_L_Up, Chl_ug_L_Down, avg_depth_m, d_bw_sondes_m, avg_m_sec, 
          pcnt_Chl_rmvd, L_hr_m2) %>% 
 dplyr::arrange(Site, Date)
  
# packages needed for imputation and randomforest regression
library(missForest)
library(rpart)
library(randomForest)
library(adabag)

# Imputation for 4 missing values (TPM & OC)
temp1 = Analysis_data[,c(1,2,3)] # temporarily remove data labels for imputation
temp2 = missForest(Analysis_data[,-c(1,2,3)])$ximp # Imputation of missing values
test = cbind(temp1,temp2) # bring back data labels 
# dimensions of data with labels removed (data only) 28 x 12 = 336 data cells
dim(Analysis_data[,-c(1,2,3)]) 
num_NA_values <- sum(is.na(Analysis_data)) # 4 missing values

colnames(test) <- c( "Site", "Date", "Experiment", "Temp","Salinity", "Turbidity",
                            "TPM", "OC", "Chl.up", "Chl.dn", 
                           "Depth","Distance", "Water velocity", "Chl.removal", 
                           "Clearance")

#Shellmaker 06-09 Neg Control, Shellmaker 05-22 Filtration and Deanze 4-17 Filtration just have astronomical outlier values for clearance.
#Lets ignore them for now as they complete dominate any model useful for comparison.
## Thea Notes ###
## Shellmaker 06-09 Neg Control - floating algae mats caused me to reposition instruments after ~10 mins into Neg Control trial. Looks like sediments never settled afterward. Huge spikes in Chl afterwards that even cutting out is suspect. Avg depth and distance b/w sondes are wrong If I just use the first part of the neg control. Going to change values in Insitu_Filter_Veolcity_All_Data.csv - will reduce clearance rate b/c reduced water volume. 

###############################################
#################### Habitat Clearance Rate Model - Random Forest
#########################

###  Use Domain knowledge to remove one single trial (NPD_2020_4_17) avg Chlup at sensor detection limit (+- 0.1 ug/L). Leave other extreme values.
test1 = test %>% 
  dplyr::filter(round(Chl.up,2) > 0.1 | round(Chl.up,2) < -0.1)

####KN edit 9-21-2021, 5-fold cross validation (repeated 1000 times) to assess tuning parameters based on SSR objective measure of fit)

#Note, I'm manually playing around with these to find a sweet spot, not going to document progression.
#Goal isn't to find best model, goal is to find competitive model.

trees=200
try = 1
split=5
cp.par = .08

SSR.sim = rep(0,200)
seed.vec = 2001:2200 # giving same numbers
for(k in 1:200){

	SSR = 0
	set.seed = seed.vec[k]
	test.ind.matrix = matrix(sample(1:25),5,5)
	for(j in 1:5){
		test.subset = test1[test.ind.matrix[j,],]
		control.subset = test1[-test.ind.matrix[j,],]
	
		control.model.rf = 			randomForest(Clearance~Temp+Salinity+Turbidity+TPM+OC+Site	,
	                        data=control.subset, # use data 		with extreme values removed 
                        ntree=trees,
                        mtry=try,
                        control=rpart.control(minsplit=		split,cp=cp.par))
		residuals = test1$Clearance[test.ind.matrix[j,]] - 		predict(control.model.rf,newdata=test.subset)
		SSR = SSR + sum(residuals^2)
	}
	SSR.sim[k] = SSR
}
mean(SSR.sim)

#mean SSR Value (cursory search only, not systematic).
#(trees,try,minsplit,cp) , mean(SSR)

#(100,3,2,.05) , 101.5m
#(200,3,2,.05) , 100.4m
#(50,3,2,.05) ,  108.5m
#(400,3,2,.05) , 106.1m

#Feel comfortable with 200 trees.

#(200,3,2,.05) , 100.4m
#(200,2,2,.05) , 97.8m
#(200,1,2,.05) , 92.3m
#(200,4,2,.05) , 104.4m

#Feel comfortable with mtry = 1

#(200,1,2,.05) , 92.3m
#(200,1,3,.05) , 84.5m
#(200,1,4,.05) , 91.9m
#(200,1,5,.05) , 97.9m

#(200,1,3,.05) , 84.5m
#(200,1,3,.02) , 90.0m
#(200,1,3,.08) , 89.3m

#Feel comfortable with cp=.05

#THEA, I only went through this once, if you want to spin your wheels you could go back through with 200,1,3,.05 as your starting point and redo ntrees, then mtry, then splits, then cp a second time (or even a third).  I think we've got most the low hanging fruit to be honest.  In general our original model was overfitting, most optimized parameters have been tuned towards less aggressive settings.

####KN edit end


############# Random Forest - WQ values affect on Filtration trails only #############

test2 <- test1[test1$Experiment=="Filtration",] # Filtration trials only
model.rf.filter = randomForest(Clearance~Temp+Salinity+Turbidity+TPM+OC+Site,
                        data=test2, # use data with extreme values removed 
                        ntree=100,
                        mtry=3,
                        control=rpart.control(minsplit=2,cp=.05))
model.rf.filter
plot(model.rf.filter)
y.hat.filter = predict(model.rf.filter,newdata=test2) # model predicted values
res.filter = test2$Clearance - y.hat.filter # residuals - difference between measured values and model predicted values 
r.sq.filter = (var(test2$Clearance) - var(res.filter))/var(test2$Clearance)
r.sq.filter # r squared value for random forest model
### R^2: 0.646

### Random Forest variable importance
Var_import <- importance(model.rf.filter)
Var_import <- Var_import[order(Var_import[,1], decreasing=TRUE),] # sort matrix highest to lowest - output names numeric
Var_import_df <- data.frame(as.list(Var_import)) # convert named numeric to data.frame
Var_import_df %<>% 
  pivot_longer(cols = c(Turbidity, Temp, TPM, Salinity, OC, Site), names_to = "Variable", values_to = "Importance") %>% 
  arrange(desc(Importance))
Var_import_df # dataframe with sorted importance values - highest to lowest
sum_import <- sum(Var_import_df$Importance)
Temp_rel_import_HCR <- (Var_import_df[[1,2]] / sum_import) * 100
Turb_rel_import_HCR <- (Var_import_df[[2,2]] / sum_import) * 100
TPM_rel_import_HCR <- (Var_import_df[[3,2]] / sum_import) * 100
OC_rel_import_HCR <- (Var_import_df[[4,2]] / sum_import) * 100
Site_rel_import_HCR <- (Var_import_df[[5,2]] / sum_import) * 100
Sal_rel_import_HCR <- (Var_import_df[[6,2]] / sum_import) * 100

```

```{r FR_Site_boxplot, echo=FALSE, fig.keep = 'all', dev = 'pdf', fig.align= "center", fig.asp=0.618, out.width="70%", fig.width=6,  message= F, fig.cap="Box plots of habitat clearance rates (HCR) during filtration trials; control trials are listed in Table 1. Each data point is the mean of a single filtration trial. HCR was not statistically different among sites (one-way Kruskal-Wallis). Filtration trials were conducted between February 2018 to June 2019 at San Rafael, CA (restored \\textit{O. lurida} reefs); Morro Bay, CA (Morro Bay Oyster Company \\textit{C. gigas} aquaculture); and Newport Bay, CA (Shellmaker and Deanza, restored beds). \\label{FR_Site_boxplot}"}

## HCR averages by Site
FR_Site <- Filter_only_outRmd_data %>% 
  group_by(Site) %>% 
  summarize(avg_HCR = mean(L_hr_m2),
            SD = sd(L_hr_m2),
            n_samples = length(L_hr_m2),
            median = median(L_hr_m2),
            varience = var(L_hr_m2))

#### one-way ANOVA -- HCR by Site
HCR_site_aov <- aov(L_hr_m2 ~ Site, data = Filter_only_outRmd_data)
#summary(HCR_site_aov)
HCR_aov_values <- broom::tidy(HCR_site_aov)

# Post-Hoc Tukey test
tukey_HCR <- TukeyHSD(HCR_site_aov)
#tukey_HCR
#plot(tukey_HCR)
#Honestly Significant Difference test
hds_HCR <- HSD.test(HCR_site_aov, trt = "Site")
#hds_HCR

########### Power analysis - sensitivity power analysis 
# one-way ANOVA effect size (n^2). n^2 = treatmentSumSquares / TotalSumSquares (TreatSS + ResidualSS) 
d_HCR_site <- { HCR_aov_values[[1,3]] / (HCR_aov_values[[1,3]] + HCR_aov_values[[2,3]]) }

# power analysis for "balanced one-way analysis of variance tests"
# For now Ted says to use the mean of the different sample sizes
HCR_site_power <- pwr.anova.test(f = d_HCR_site, 
                                 k = 4,
                                 n = mean(FR_Site$n_samples), 
                                 sig.level = 0.05,
                                 power = NULL) # solve for this

# test if shellmaker extreme value was removed. SD is approximately equal among sites
x <- Filter_only_outRmd_data %>% 
  filter(L_hr_m2 > -1000) %>% 
  group_by(Site) %>% 
  summarize(avg_L_hr_m2 = mean(L_hr_m2),
            SD = sd(L_hr_m2),
            n_samples = length(L_hr_m2))

####### Try Kruskal- Wallis one-way analysis of variance 
## non-parametric equvilant to one-way ANOVA
# Assumtions 1) continuous response variable 2) Independent groups 3) Distributions have similar shapes

# K-W test - tidy converts to clean dataframe
KW_HCR_site <- broom::tidy(kruskal.test(L_hr_m2 ~ Site, data = Filter_only_outRmd_data))

# Significant results can then use VVV pairwise comparison between groups
# pairwise.wilcox.test(PlantGrowth$weight, PlantGrowth$group, p.adjust.method = "BH")

### Filtration by site (Filtration Trials only)
FR_box <- ggplot((Filter_only_outRmd_data), aes(Site, L_hr_m2)) +
  geom_boxplot(color = Site_colors) +
  geom_dotplot(binaxis = "y", stackdir = "center", aes(fill = Site),
               alpha = 0.5, dotsize = 0.65, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = Hab_FR_Label)

plot(FR_box) 

```

## Habitat Clearance Rates 

Mean HCR at San Rafael was `r round(mean(SR_filter_sum$L_hr_m2),1)` L hr^-1^ m^-2^ (*N* = `r nrow(SR_filter_sum)`, *SD* = `r round(sd(SR_filter_sum$L_hr_m2),1)`), `r round(mean(MB_filter_sum$L_hr_m2),1)` L hr^-1^ m^-2^ (*N* = to `r nrow(MB_filter_sum)`, *SD* = `r round(sd(MB_filter_sum$L_hr_m2),1)`) at Morro Bay, 
`r round(mean(NPSM_filter_sum$L_hr_m2),1)` L hr^-1^ m^-2^ (*N* = `r nrow(NPSM_filter_sum)`, *SD* = `r round(sd(NPSM_filter_sum$L_hr_m2),1)`) at Shellmaker, 
and `r round(mean(NPD_filter_sum$L_hr_m2),1)` L hr^-1^ m^-2^ (*N* = `r nrow(NPD_filter_sum)`, *SD* = `r round(sd(NPD_filter_sum$L_hr_m2),1)`) at Deanza (Figure \ref{FR_Site_boxplot}).
Control trials measured `r round(SR_NC_sum$L_hr_m2,1)`, `r round(MB_NC_sum$L_hr_m2,1)`, `r round(NPSM_NC_sum$L_hr_m2,1)`, and `r round(NPD_NC_sum$L_hr_m2,1)` L hr^-1^ m^2^ at San Rafael, Morro Bay, Shellmaker, and Deanza respectively.
HCR for filtration trials did not differ significantly among sites, see Figure \ref{FR_Site_boxplot} (one-way Kruskal-Wallis, *p* = `r round(KW_HCR_site$p.value,2)`).
The upper 0.5 quantile of HCR at each site, representing the top filtration performance within each site, also did not differ among sites (one-way Kruskal-Wallis, *p* = `r round(KW_HCR50_site$p.value,2)`).
Individual water quality variables did not significantly relate with HCR at 0.5 and 0.9 quantiles: temperature ($\tau$ = 0.5, $\beta$±*SE* = `r round(sumQRtemp[[1]]$coefficients[2,2],2)`, *p* = `r round(sumQRtemp[[1]]$coefficients[2,4],2)`; $\tau$ = 0.9, $\beta$±*SE* = `r round(sumQRtemp[[2]]$coefficients[2,2],2)`, *p* = `r round(sumQRtemp[[2]]$coefficients[2,4],2)`), salinity ($\tau$ = 0.5, $\beta$±*SE* = `r round(sumQRsal[[1]]$coefficients[2,2],2)`, *p* = `r round(sumQRsal[[1]]$coefficients[2,4],2)`; $\tau$ = 0.9, $\beta$±*SE* = `r round(sumQRsal[[2]]$coefficients[2,2],2)`, *p* = `r round(sumQRsal[[2]]$coefficients[2,4],2)`),
turbidity ($\tau$ = 0.5, $\beta$±*SE* = `r round(sumQRturb[[1]]$coefficients[2,2],2)`, *p* = `r round(sumQRturb[[1]]$coefficients[2,4],2)`; $\tau$ = 0.9, $\beta$±*SE* = `r round(sumQRturb[[2]]$coefficients[2,2],2)`, *p* = `r round(sumQRturb[[2]]$coefficients[2,4],2)`),
TPM ($\tau$ = 0.5, $\beta$±*SE* = `r round(sumQRtpm[[1]]$coefficients[2,2],2)`, *p* = `r round(sumQRtpm[[1]]$coefficients[2,4],2)`; $\tau$ = 0.9, $\beta$±*SE* = `r round(sumQRtpm[[2]]$coefficients[2,2],2)`, *p* = `r round(sumQRtpm[[2]]$coefficients[2,4],2)`),
OC ($\tau$ = 0.5, $\beta$±*SE* = `r round(sumQRoc[[1]]$coefficients[2,2],2)`, *p* = `r round(sumQRoc[[1]]$coefficients[2,4],2)`; $\tau$ = 0.9, $\beta$±*SE* = `r round(sumQRoc[[2]]$coefficients[2,2],2)`, *p* = `r round(sumQRoc[[2]]$coefficients[2,4],2)`) (Figure \ref{Quantile_Reg}).

```{r Final_Summary_table, eval=FALSE, include=FALSE, results='asis'}

## Having hard time compiling this table in Rmd. LaTeX output in Console runs perfectly in Overleaf. ## Need to build table in Word per request of CSUF Thesis Reader :(
## Export to pdf from Overleaf, Open pdf in Word

#library(xtable)

# Organize & clean summary table data
Final_table <- Master_analysis_table %>%
  mutate(Date = mdy(Date),
         Site = ifelse(Site == "San Rafael", "SR",
                       ifelse(Site == "Morro Bay", "MB",
                              ifelse(Site == "Shellmaker", "NPSM",
                                     ifelse(Site == "Deanza", "NPD",  "Pattern Not Found")))),
         Experiment = ifelse(grepl("Neg Control*", Master_analysis_table$Experiment), 
                             gsub("Neg Control*", "Ctrl", Master_analysis_table$Experiment), 
                             "Fltr"),
         L_hr_m2 = round(L_hr_m2),
         pcnt_Chl_rmvd = round(pcnt_Chl_rmvd, digits = 1), ### Get rid of this 
         Temp_C_Up = round(Temp_C_Up, digits = 2),
         Sal_ppt_Up = round(Sal_ppt_Up, digits = 1),
         Turbidity_NTU_Up = round(Turbidity_NTU_Up, digits = 1),
         Avg_TPM_mg_L = round(Avg_TPM_mg_L, digits = 4),
         Avg_OC_Ratio = round(Avg_OC_Ratio, digits = 2),
         Chl_ug_L_Up = round(Chl_ug_L_Up, digits = 1),
         Chl_ug_L_Down = round(Chl_ug_L_Down, digits = 1),
         avg_depth_m = round((avg_depth_cm/100), digits = 2),
         d_bw_sondes_m = round(d_bw_sondes_m, digits = 1),
         avg_m_sec = round(avg_m_hr/3600, 2)) %>%
  dplyr::select(Site, Date, Experiment, Tide, L_hr_m2, pcnt_Chl_rmvd, Chl_ug_L_Up, Chl_ug_L_Down,Temp_C_Up, Sal_ppt_Up, 
                Turbidity_NTU_Up, Avg_TPM_mg_L, Avg_OC_Ratio, 
                avg_depth_m, d_bw_sondes_m, avg_m_sec) %>% 
 dplyr::arrange(Site, Date)


# arrange by site latitude
Final_table <- arrange(transform(Final_table, Site = factor(Site,levels = site_by_lat_abv)), Site)

##########################################
## LaTeX Table broken. Kevin is taking Final_table csv and constructing LaTeX table outside of this code for publication. - AM 2021_7_21
#########################################

data_removed <- Final_table[19,] # name excluded data
data_analysis <- Final_table[-19,] # remove excluded analysis data points

Latex_table_data <- rbind(data_analysis, data_removed) # add excluded data back in at bottom 

### Add to column names for LaTeX to read a multicolumn names
#\\multicolumn{1}{p{1cm}}{\\centering x \\\ y}
# Make date formated as character to display properly
Latex_table_data$Date <- as.character(Latex_table_data$Date)
colnames(Latex_table_data) <- c( "\\multicolumn{1}{p{1cm}}{\\centering Site}", 
                            "\\multicolumn{1}{p{1cm}}{\\centering Date}", 
                            "\\multicolumn{1}{p{1cm}}{\\centering Trial}",
                            "\\multicolumn{1}{p{1cm}}{\\centering Tide}",
                           "\\multicolumn{1}{p{1cm}}{\\centering Chl\\textsubscript{rmd} \\\ (\\%)}",
                           "\\multicolumn{1}{p{1cm}}{\\centering HCR \\\ (L/hr/$m^{2}$)}",
                            "\\multicolumn{1}{p{1cm}}{\\centering Temp \\\ (\\si{\\degree}C)}",
                            "\\multicolumn{1}{p{1cm}}{\\centering Sal \\\ (ppt)} ", 
                            "\\multicolumn{1}{p{1cm}}{\\centering Turb \\\ (NTU)}",
                            "\\multicolumn{1}{p{1cm}}{\\centering TPM \\\ (mg/L)}", 
                            "\\multicolumn{1}{p{1cm}}{\\centering OC \\\ (ratio)}", 
                            "\\multicolumn{1}{p{1cm}}{\\centering Chl\\textsubscript{up} \\\ ($\\upmu$g/L)}", 
                           "\\multicolumn{1}{p{1cm}}{\\centering Depth \\\ (m)}",
                           "\\multicolumn{1}{p{1cm}}{\\centering Dist \\\ (m)}",
                           "\\multicolumn{1}{p{1cm}}{\\centering Vel \\\ (m/s)}")

# create xtable
sumtable_latex <- xtable(Latex_table_data, 
                         caption = "Details of Whole-habitat In Situ Filtration and Control Trials Across Four California Sites. All Values are Means from the Entire Trial. The Trial Separated at the Bottom is within the Chlorophyll  Sensor’s Error and was Removed from the Analysis. Dashes Denote Missing Values.",
                         label = "tb:summary",
                         align = "lllllrrrrrrrrrrr",
                         type = "latex")

#foot_note <-list(pos = list(0), command = NULL)
#foot_note$pos[[1]] <- c(nrow(sumtable_latex))
#foot_note$command <- c("\\hline\\footnotesize{SR: San Rafael, MB: Morro Bay, NPSM: Newport Shellmaker, NPD: Newport Deanza, Fltr: Filtration, Ctrl: Control, Chl\\textsubscript{rmd}: Percent Chlorophyll $\alpha$ Removed, HCR: habitat clearance rate, Temp: temperature, Sal: salinity, Turb: turbidity, TPM: total particulate matter, OC: organic content, Dist: Distance between instruments, Vel: water velocity.}\n")

print.xtable(sumtable_latex, 
      booktabs=TRUE, 
      caption.placement="top", 
      NA.string = NA, 
      sanitize.colnames.function=function(x){x},
      floating.environment = "sidewaystable",
      #add.to.row = foot_note,
      hline.after = c(-1,0,25,26),
      comment = F,
      include.rownames = FALSE,
      timestamp = NULL,
      size = "\\fontsize{10pt}{10pt}\\selectfont") # font size / line spacing 


### Table abbreviations description

#Field sites are San Rafael (SR), Morro Bay (MB), Newport Shellmaker (NPSM), and Newport Deanza (NPD). Trials are either filtration over oyster habitat (Fltr), or control over mudflat (Ctrl). Chl$\textsubscript{rmd}$ is the percent chlorophyll $\alpha$ removed as water traveled across the habitat. HCR is the habitat clearance rate originally described in Milbrandt et al. (2015). Temp is temperature, Turb. is Turbidity, TPM is sesson total particulate matter, OC is relative organic content of TPM. Chl$\textsubscript{up}$ is chlorophyll $\alpha$ concentration at the upstream instrument. Depth is the average water column depth, Dist. is the distance between instruments, and Vel. is the average water velocity.
```

\includepdf[pagecommand={},noautoscale = true]{Summary_Table_2020_11_5.pdf}

\captionsetup[table]{labelformat=empty}

A random forest regression containing only filtration trials (*R*^2^ = `r round(r.sq.filter, 2)`) indicated that temperature (`r round(Temp_rel_import_HCR,1)`\%) had the highest relative importance to the model, followed by turbidity (`r round(Turb_rel_import_HCR,1)`\%), TPM (`r round(TPM_rel_import_HCR,1)`\%), OC (`r round(OC_rel_import_HCR,1)`\%), site (`r round(Site_rel_import_HCR,1)`\%), and salinity (`r round(Sal_rel_import_HCR,1)`\%).

```{r TPM_OC_Values, include=FALSE}
TPM_OC_Values <- Master_analysis_table %>% 
  dplyr::select(Site, Avg_TPM_mg_L, Avg_OC_Ratio) %>% 
  na.omit()

SR_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "San Rafael")

MB_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "Morro Bay")

NPSM_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "Shellmaker")

NPD_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "Deanza")

NP_combo_TPM_OC <- TPM_OC_Values %>% 
  filter(Site == "Deanza" | Site == "Shellmaker")
```

## Seston Quantity and Quality

Total Particulate Matter (TPM) was significantly related to seston OC when sites were combined  (Figure \ref{TPM_OC_model}). 
This model includes all field days (filtration and controls) because water samples used to determine TPM and OC were collected the same way regardless of the trial. 
Northern San Francisco Bay (San Rafael) TPM averaged `r round(mean(SR_TPM_OC$Avg_TPM_mg_L),2)` mg/L (*N* = `r nrow(SR_TPM_OC)`, *SD* = `r round(sd(SR_TPM_OC$Avg_TPM_mg_L),2)`), and Morro Bay TPM averaged `r round(mean(MB_TPM_OC$Avg_TPM_mg_L),2)` mg/L (*N* = `r nrow(MB_TPM_OC)`, *SD* = `r round(sd(MB_TPM_OC$Avg_TPM_mg_L),2)`) (Figure \ref{TPM_OC_model}).
Newport Bay (Deanza and Shellmaker) TPM averaged `r round(mean(NP_combo_TPM_OC$Avg_TPM_mg_L),2)` mg/L (*N* = `r nrow(NP_combo_TPM_OC)`, *SD* = `r round(sd(NP_combo_TPM_OC$Avg_TPM_mg_L),2)`).
Northern San Francisco Bay (San Rafael) OC averaged `r round(mean(SR_TPM_OC$Avg_OC_Ratio),2)` (*N* = `r nrow(SR_TPM_OC)`, *SD* = `r round(sd(SR_TPM_OC$Avg_OC_Ratio),2)`), and Morro Bay OC averaged `r round(mean(MB_TPM_OC$Avg_OC_Ratio),2)` (*N* = `r nrow(MB_TPM_OC)`, *SD* = `r round(sd(MB_TPM_OC$Avg_OC_Ratio,2))`).
Newport Bay (Deanza and Shellmaker) OC averaged `r round(mean(NP_combo_TPM_OC$Avg_OC_Ratio),3)` (*N* = `r nrow(NP_combo_TPM_OC)`, *SD* = `r round(sd(NP_combo_TPM_OC$Avg_OC_Ratio),2)`) (Figure \ref{TPM_OC_model}).

## Filter Feeding Community

```{r Oly_DTW_Shell_length_setup, include=FALSE}
############ Olympia DTW and Shell Length #####################
# Determine what quadrat samples have missing data (NAs)
NPD_biv_bio_summary <- NPD_bivalves %>% 
  group_by(excavation_sample) %>% 
  summarize(avg_DTW = mean(tissue_dry_weight_g))

# what are the unique species in dataset
#unique(NPD_bivalves$species)

#Newport, CA All excavation quadrat samples processed to date aggregated (summer 2020)

# Still contains missing data --> which quadrats to use for analyses
NPD_bivalve_data_clean <- NPD_bivalves %>% 
  filter(!excavation_sample %in% c("Q10", "Q4", "Q8")) %>% 
  mutate(species = ifelse(grepl("adula", ignore.case = T, species), "Adula diegensis",
                          ifelse(grepl("muscul.", ignore.case = T, species), "Musculista senhousia",
                                 ifelse(grepl("myt", ignore.case = T, species), "Mytilus galloprovincialis",
                                        ifelse(grepl(".lurida", ignore.case = T, species), "Ostrea lurida",
                                               ifelse(grepl("mussel", ignore.case = T, species), "Unknown mussel",
                                                      ifelse(grepl(".spinosum", ignore.case = T, species), "Crucibulum spinosum",
                                                       "Pattern Not Found"))))))) %>% 
  # Filter out unclassified/ empty species & samples less than 0g (processing error, doesn't make sense)
  filter(!species %in% c("Pattern Not Found", "Crucibulum spinosum"))

# Remove outlier and negative DTW value
NPD_bivalve_outrmd <- NPD_bivalve_data_clean %>%
  mutate(DTW_outlier = find_outlier(NPD_bivalve_data_clean$tissue_dry_weight_g)) %>% 
  filter(DTW_outlier == FALSE & tissue_dry_weight_g >= 0)

# All excavation quadrat samples processed to date aggregated (summer 2020)
# filter only O. lurida dtw in newport
NPD_oly_outRmd <- NPD_bivalve_outrmd %>% 
  filter(species == "Ostrea lurida") %>% 
  drop_na() %>% 
  as_tibble()
```

```{r Bivalve_Den_Setup, include=FALSE}
################### Bivalve Density by Site #########################
# read in bivalve density data from cleaned data file
Bivalve_Den_data <- fread(file.path(Bivalve_output_directory, "Bivalve_Density_summary.csv"))

# replace NAs with 0s
Bivalve_Den_data[is.na(Bivalve_Den_data)] <- 0

Bivalve_Den_data <- Bivalve_Den_data %>% 
  # move species from individual columns to a single column with new density column to contain data
  pivot_longer(c(Adula_diegensis_m2, Musculista_senhousia_m2, Mytilus_galloprovincialis_m2,
                 Ostrea_lurida_m2, Crassostrea_gigas_m2, Argopecten_ventricosa_m2, 
                 Geukensia_demissa_m2, Crassostrea_gigas_m2), 
               names_to = "Species", 
               values_to =  "individuals_m2") %>% 
  # remove "_m2" from species names in Species column
  mutate(Species = str_replace(Species, "_m2", ""),
         Species = str_replace(Species, "_", " "))

#Reorder levels of Site factor - display North to South
order_by_site <- c("San Rafael", "Morro Bay", "Shellmaker", "Deanza") 
Bivalve_Den_data <- arrange(transform(Bivalve_Den_data, Site = factor(Site,levels = order_by_site)), Site)
# Density values for in text
SR_Den <- Bivalve_Den_data %>% 
  filter(Site == "San Rafael",
         individuals_m2 > 0) %>% 
  summarise(total_den = sum(individuals_m2))

MB_Den <- Bivalve_Den_data %>% 
  filter(Site == "Morro Bay",
         individuals_m2 > 0) %>% 
  summarise(total_den = sum(individuals_m2))

NPSM_Den <- Bivalve_Den_data %>% 
  filter(Site == "Shellmaker",
         individuals_m2 > 0) %>% 
  summarise(total_den = sum(individuals_m2))
  
NPD_Den <- Bivalve_Den_data %>% 
  filter(Site == "Deanza",
         individuals_m2 > 0) %>% 
  summarise(total_den = sum(individuals_m2))
## Species list
NPSM_species <- Bivalve_Den_data %>% 
  filter(Site == "Shellmaker",
         individuals_m2 > 0) %>% 
  group_by(Species) %>% 
  summarise(species_den = individuals_m2)
 
NPD_species <- Bivalve_Den_data %>% 
  filter(Site == "Deanza",
         individuals_m2 > 0) %>% 
  group_by(Species) %>% 
  summarise(species_den = individuals_m2)
  
```

In November 2017 the estimated bivalve density at San Rafael was `r SR_Den` individuals/m^2^, all of which were *Ostrea lurida* (Figure \ref{Bivalve_Den}).
Other bivalves were noted, but were rare, and were not detected in sample bags (C. Zabin, unpublished data). 
Morro Bay had an estimated `r MB_Den` *Crassostrea gigas* individuals/m^2^ in the summer of 2018 (Morro Bay Oyster Company); personal field observations confirm the lack of bivalve fouling on the aquaculture lines.
In May 2018, Shellmaker had an estimated `r NPSM_Den` individuals/m^2^, composed of *Adula diegensis* (`r NPSM_species[NPSM_species$Species=="Adula diegensis",2]` individuals/m^2^), *Musculista senhousia* (`r NPSM_species[NPSM_species$Species=="Musculista senhousia",2]` individuals/m^2^), *O. lurida* (`r NPSM_species[NPSM_species$Species=="Ostrea lurida",2]` individuals/m^2^), *Mytilus galloprovincialis* (`r NPSM_species[NPSM_species$Species=="Mytilus galloprovincialis",2]` individuals/m^2^), *Geukensia demissa* (`r NPSM_species[NPSM_species$Species=="Geukensia demissa",2]` individuals/m^2^), and *Argopecten ventricosa* (`r NPSM_species[NPSM_species$Species=="Argopecten ventricosa",2]` individuals/m^2^) (Figure \ref{Bivalve_Den}).
Deanza had an estimated `r NPD_Den` individuals/m^2^ in May 2018, and was composed of *M. senhousia* (`r NPD_species[NPD_species$Species=="Musculista senhousia",2]` individuals/m^2^), *A. diegensis* (`r NPD_species[NPD_species$Species=="Adula diegensis",2]` individuals/m^2^), *O. lurida* (`r NPD_species[NPD_species$Species=="Ostrea lurida",2]` individuals/m^2^), *M. galloprovincialis* (`r NPD_species[NPD_species$Species=="Mytilus galloprovincialis",2]` individuals/m^2^) (Figure \ref{Bivalve_Den}).

```{r Bivalve_Den_bargraph, echo=FALSE, fig.align="center", fig.asp=0.618, fig.width=6, warning= F, message= F, fig.keep='all', dev='pdf', out.width="70%", fig.cap="Bivalve community density and composition for each study site. San Rafael data were collected November 2017 by the Zabin lab at SERC, Morro Bay was estimated by Morro Bay Oyster Company in 2018, and Shellmaker and Deanza were surveyed in May 2018 by the Zacherl lab at CSUF.  \\label{Bivalve_Den}"}

#unique(Bivalve_Den_data$Species) # list each individual bivalve species
# Assign species to specific color
Species_colors <- c("Adula diegensis" = "#8D8680",
                    "Argopecten ventricosa" = "#CCC3C5",
                    "Crassostrea gigas" = "#6D6340",
                    "Geukensia demissa" = "#524D4F",
                    "Musculista senhousia" = "#B0915B",
                    "Mytilus galloprovincialis" = "#7E4B41",
                    "Ostrea lurida" = "#9986A5",
                    "Unknown mussel" = "black")

# Stacked bar graph
Bivalve_Den_graph <- ggplot(Bivalve_Den_data, aes(Site)) + 
  geom_col(aes(x = Site, y = individuals_m2, fill = Species)) +
 # geom_text(aes(label = round(Bivalve_den_m2,0), y = Bivalve_den_m2), 
          #  vjust = -0.5) +
  theme_classic() +
  theme(legend.text = element_text(face = "italic")) +
  scale_fill_manual(values = Species_colors) +
  scale_y_continuous(breaks = seq(0, max(Bivalve_Den_data$Bivalve_den_m2), by = 500)) +
  labs(y = expression("Individuals m"^-2))
Bivalve_Den_graph
```

```{r Oly_DTW_SL_Newport_Yaquina_SanDiego_data, include=FALSE}
########### import Matt Gray DTW dataset - Yaquina Bay, Oregon
Gray_Oly_Data <- fread(file.path('./Data/bivalve_density_community', "MattGray_2011_2013_Olurida_allometric_data.csv"))
colnames(Gray_Oly_Data) <- c("ID", "Date", "species", "length_mm", "shell_tissue_g", "vial_weight", "Vial_wet_oyster", "Tissue_weight_g", "vial_dry_oyster", "tissue_dry_weight_g")
Gray_Oly_Data %<>% 
  mutate(species = ifelse(species %in% "O. lurida", "Ostrea lurida", "Pattern Not FOund"),
         Site = "Yaquina")

Gray_Oly_SL_DTW <- Gray_Oly_Data %>% 
  dplyr::select(Date, Site, species, length_mm, tissue_dry_weight_g)

##### Import Jason Langevin DTW dataset - San Diego Bay, California
SD_Oly_SL_DTW <- read_csv("./Data/bivalve_density_community/Langevin_SD_O_lurida_length_weight_data.csv")
SD_Oly_SL_DTW %<>% 
  rename("Tidal_hight" = "Tidal height",
         "length_mm" = "Length (mm)",
         "tissue_dry_weight_g" = "Dy (g)") %>% 
  mutate("species" = "Ostrea lurida") %>% 
  dplyr::select(Site, species, length_mm, tissue_dry_weight_g)

#### Import Newport Deanza (NPD) O. lurida dataframe cleaned above

NPD_Oly_SL_DTW <- NPD_oly_outRmd %>% 
  mutate(Date = excavation_date) %>% 
  dplyr::select(Date, Site, species, length_mm, tissue_dry_weight_g)

# join Newport, Yaquina, and San Diego data together into singe dataframe
Oly_combo_DTW_SL <- full_join(NPD_Oly_SL_DTW,Gray_Oly_SL_DTW) %>% 
  full_join(SD_Oly_SL_DTW) %>% 
  mutate(legend = case_when(Site == "Deanza" & species == "Ostrea lurida" ~ "Deanza",
                            Site == "Yaquina" & species == "Ostrea lurida" ~ "Yaquina",
                            Site == "San Diego" & species == "Ostrea lurida" ~ "San Diego"))

#Oly_combo_DTW_SL$legend <- fct_relevel(Oly_combo_DTW_SL$legend, "Yaquina", "San Diego", "Deanza")
Oly_combo_DTW_SL <- Oly_combo_DTW_SL %>% 
  mutate(Site = fct_relevel(Site, "San Diego", "Deanza", "Yaquina"))

#### start of Kevin's code ###
#find domain of shell length

#fit separate models log(weight)~log(length)

#come up with 1000 evenly separated x values over domain.  Compute y.hat for each of the three models
#add to plot.

temp = Oly_combo_DTW_SL

#find domain of shell lenght

x.seq = seq(min(temp$length_mm),max(temp$length_mm),by=0.05)

#fit separate models

x1 = temp$length_mm[temp$Site == "Deanza"]
y1 = temp$tissue_dry_weight_g[temp$Site=="Deanza"]
x2 = temp$length_mm[temp$Site == "San Diego"]
y2 = temp$tissue_dry_weight_g[temp$Site=="San Diego"]
x3 = temp$length_mm[temp$Site == "Yaquina"]
y3 = temp$tissue_dry_weight_g[temp$Site=="Yaquina"]

lx1 = log(x1)
lx2 = log(x2)
lx3 = log(x3)
ly1 = log(y1)
ly2 = log(y2)
ly3 = log(y3)

m1 = lm(ly1~lx1)
m2 = lm(ly2~lx2)
m3 = lm(ly3~lx3)

#example with confidence bounds
#y.seq1 = exp(predict(m1,newdata=data.frame(lx1=log(x.seq)),interval="confidence"))
y.seq1 = exp(predict(m1,newdata=data.frame(lx1=log(x.seq))))
y.seq2 = exp(predict(m2,newdata=data.frame(lx2=log(x.seq))))
y.seq3 = exp(predict(m3,newdata=data.frame(lx3=log(x.seq))))

temp1 = data.frame(x.seq,y.seq1)
temp2 = data.frame(x.seq,y.seq2)
temp3 = data.frame(x.seq,y.seq3)
### End of Kevin's code ####

######## Graph with Deanza, Yaquina & San Diego DTW x Shell length

Oly_combo_DTW_SL <- Oly_combo_DTW_SL %>% 
  arrange(Site, "San Diego", "Deanza", "Yaquina")

Combo_Oly_shell_DTW_figure <- ggplot(Oly_combo_DTW_SL, aes(length_mm, tissue_dry_weight_g)) +
  geom_point(aes(color = Site, shape = Site)) +
  scale_color_manual(labels = c("San Diego Bay, CA",
                                "Deanza, Newport Bay, CA",
                                "Yaquina Bay, OR"),
                     values = c("San Diego" = "#FCA636FF",
                                "Deanza" = "#440154FF",
                                "Yaquina" = "#26828EFF"), #9986A5
                     na.translate = TRUE, drop = FALSE) + 
  scale_shape_manual(labels = c("San Diego Bay, CA",
                                "Deanza, Newport Bay, CA",
                                "Yaquina Bay, OR"),
                     values = c("San Diego" = 6,
                                "Yaquina" = 3,
                                "Deanza" = 16),
                     na.translate = TRUE, drop = FALSE) + 
  theme_classic() +
  theme(legend.box.background = element_rect(colour = "black"),
        legend.justification = c(0, 1), 
        legend.position = c(0.075, 0.95),
        legend.title = element_blank()) +
  ylim(0,1)+
  labs(x = 'Shell Length (mm)',
       y = 'Dry Tissue Weight (g)')+
  geom_line(data=temp1,aes(x.seq,y.seq1), color = "#440154FF") + #9986A5
  geom_line(data=temp2,aes(x.seq,y.seq2), color = "#FCA636FF") +
  geom_line(data=temp3,aes(x.seq,y.seq3), color = "#26828EFF") #20A386FF


Combo_Oly_shell_DTW_figure

ggsave(Combo_Oly_shell_DTW_figure, height=3.708, width=6, filename = "Oly_DTW_SL_allsites.png")
#fig.asp=0.618

```


```{r Bivalve_biomass_setup, include=FALSE}
######### Biomass by Site ############################
# excavation quadrat area m^2 (0.25 x 0.25m)
NP_excavation_quad_area = 0.0625

### vvv incorrect calculation vvv - mean of quadrat DTW instead of total
# calculate DTW / m^2 / species 
#NPD_biv_DTW_m2 <- NPD_bivalve_outrmd %>% 
 # group_by(excavation_sample, species) %>% 
  #summarize(mean_DTW = mean(tissue_dry_weight_g)) %>% # show the average DTW for each species within each excav sample
  #ungroup() %>% # undo previous grouping by excavation_sample and species
  #group_by(species) %>% # new grouping by species to determine the average DTW by species across all quads
  #summarize(mean_DTW_m2 = mean(mean_DTW/NP_excavation_quad_area)) %>% 
  #mutate(Site = "Deanza") %>%  # add new column with site info for bar graph
  #rename(Species = species) # rename column with capital case
 ###### ^^^^^ end of incorrect code ^^^ 

#### Check above code for calculations
a <- NPD_bivalve_outrmd %>% 
  group_by(excavation_sample, species) %>% 
  summarize(total_quad_dtw = sum(tissue_dry_weight_g))
b <- a %>% 
  group_by(species) %>% 
  summarize(mean_quad_dtw = mean(total_quad_dtw))
NPD_biv_DTW_m2 <- b %>% 
  mutate(mean_DTW_m2 = mean_quad_dtw/NP_excavation_quad_area) %>% # adjust by quadrat area
  mutate(Site = "Deanza") %>%  # add new column with site info for bar graph
  rename(Species = species) %>% # rename column with capital case
  dplyr::select(Site, Species, mean_DTW_m2)

## Values to populated text
NPD_total_DTW <- NPD_biv_DTW_m2 %>% 
  summarise(total = sum(mean_DTW_m2))

NPD_species_DTW <- NPD_biv_DTW_m2 %>% 
  group_by(Species) %>% 
  summarise(total = mean_DTW_m2)

```


Direct biomass data were only available for Deanza, which estimated `r round(NPD_total_DTW, 2)` g/m^2^ of bivalve dry tissue weight (DTW) (Figure \ref{Bivalve_DTW_Biomass}). 
*O. lurida* had the highest DTW with `r round(NPD_species_DTW[NPD_species_DTW$Species=="Ostrea lurida",2],2)` g/m^2^, followed by *M. galloprovincialis* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Mytilus galloprovincialis",2],)` g/m^2^), an unknown *Modiolus* sp. (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Unknown mussel",2],2)` g/m^2^), *A. diegensis* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Adula diegensis",2],2)` g/m^2^), and *M. senhousia* (`r round(NPD_species_DTW[NPD_species_DTW$Species=="Musculista senhousia",2],2)` g/m^2^) (Figure \ref{Bivalve_DTW_Biomass}).
San Rafael surveys did not directly measure biomass, but did include *O. lurida* shell length measurements which I used to estimate DTW (Figure \ref{Bivalve_DTW_Biomass}) with a model describing the relationship between *O. lurida* shell length and DTW at Deanza (ln(y) = `r round(coef(OlyDTW_loglog_sum)[1,1],digits=2)` `r round(coef(OlyDTW_loglog_sum)[2,1],digits=3)`ln(x), R^2^ = `r round(OlyDTW_loglog_sum$r.squared,digits=2)`, *p* < 0.0001) (Figure \ref{NPD_Oly_DTW_SH_Trans}). 
I estimated San Rafael had `r round(SR_species_DTW$total,2)` g/m^2^ of *O. lurida* (Figure \ref{Bivalve_DTW_Biomass}).
A direct comparison between site biomass to HCR was not possible with my limited biomass and shell length data. 

```{r Bivalve_biomass_bargraph, echo=FALSE, fig.align="center", fig.asp=0.618, fig.width=6, warning= F, message= F, fig.keep='all', dev='pdf', out.width="70%", fig.cap="Estimated biomass of \\textit{O. lurida} habitat at Deanza and San Rafael sites. Data for Deanza was collected by the Zacherl lab (CSUF) in May 2018, the relationship between \\textit{O. lurida} shell length and dry tissue weight (DTW) from the survey was used to estimate DTW at San Rafael with shell length data collected by the Zabin lab (SERC) in November 2017. \\label{Bivalve_DTW_Biomass}"}

####### Bar graph NPD Biomass / DTW by species
Bivalve_Biomass_graph <- ggplot(NPD_biv_DTW_m2, aes(Site)) + 
  geom_col(aes(x = Site, y = mean_DTW_m2, fill = Species)) +
  theme_classic() +
  scale_fill_manual(values = Species_colors) +
  theme(legend.text = element_text(face = "italic")) +
  #scale_y_continuous(breaks = seq(0, max(NPD_Posbiomass_outlier$tissue_dry_weight_g), by = 500)) +
  labs(y = expression("Dry Tissue Weight (g m" ^-2* ")"))
## Add total Biomass on top of bars
Bivalve_Biomass_graph
```

```{r Estimate_filt_w_other_studies}
##### Compare previous filtration studies to our HCR findings

## Gray & Langdon (2018)
# O. lurida clearance rate mean L/hr/g (DTW) Yaquina Bay, OR
gray_dry_cr_oly <- 0.78 # Dry season
gray_wet_cr_oly <- 0.19 # Wet season
# O. lurida all season model insitu - as selected by stepwise regression analysis of temp, salinity, total particulate matter, organic content. 
estimate_oly_cr_gray <- function (temp, OC){
  clearance = -0.95 + 2.49*OC + 0.06*Temp
  return(clearance)
  }
# M. gigas clearance rate mean L/hr/g (DTW)
gray_dry_cr_gigas <- 0.95
gray_wet_cr_gigas <- 1.06

## Wheat & Ruesink (2013)
# M gigas clearance rate L/hr/g grown on long-lines in Willipa Bay, WA
wheat_cr_gigas <- 0.73

## Deanza O.lurida biomass per m^2 of habitat
npd_dtw_oly <- NPD_species_DTW[NPD_species_DTW$Species=="Ostrea lurida",2]

# estimate clearance rate with NPD biomass & Gray's model
NPD_oly_cr_est <- estimate_oly_cr_gray()

```

## Supplemental / Repository 


```{r HCR_Quantile_Regression, fig.width=7, fig.asp=1.2, warning=F, message = F, fig.cap="The relationship between water quality variables and habitat clearance rate (HCR). For all oyster habitat filtration trials, quantile regression with $\\tau$ = 0.5 and $\\tau$ = 0.9 was used to test wheather HCR changed with A) temperature, B) salinity, C) turbidity, D) total particulate matter, or E) organic content; slopes that are not significantly different from zero are indicated by red dashed lines. Dotted gray lines are the mean value of HCR. \\label{Quantile_Reg}"}
#create model variables
attach(Filter_only_outRmd_data)
qr_x <- cbind(Temp_C_Up, Sal_ppt_Up, Turbidity_NTU_Up, Avg_TPM_mg_L, Avg_OC_Ratio, Site)
#summary(qr_x)
qr_y <- cbind(L_hr_m2)
#summary(qr_y)

## OLS Model
HCR_lm <- lm(qr_y ~ qr_x)
#summary(HCR_lm)
#hist(Filter_only_outRmd_data$L_hr_m2)
# Look at linear model assumptions
# Doesn't look good with this data
#par(mfrow=c(2,2))
#plot(HCR_lm)

##########################################
############## Quantile Regression ###############
## Bivariate comparisons only, not doing multivariate in this study (Talked with Ted Grosholz)
library(quantreg) # Quantile Rregression package

#### Look at change over most Quantiles
qr_temp <- rq(L_hr_m2 ~ Temp_C_Up,
             tau = seq(0.05,0.95, by = 0.05),
             na.action = na.omit,
             data = Filter_only_outRmd_data)
#summary(qr_temp)
#plot(qr_temp)

#### Temperature
## Looking at .5 and .9 quantiles of Habitat Clearance Rates - Filtration most likely on
qr_5_9_temp <- rq(L_hr_m2 ~ Temp_C_Up,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRtemp <- summary(qr_5_9_temp, se = "boot", R = 1000)
#sumQRtemp

## Graph
# if statements choose linetype - solid (significant) or dashed (not significant)
qrplot_temp <- ggplot(Filter_only_outRmd_data, aes(Temp_C_Up, L_hr_m2)) +
  geom_point(aes(color = Site)) +
  # gray line indicating mean of HCR
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
  labs(x = expression(paste("Temperature (", degree, "C)")),
       y = "",
       title = "A)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRtemp[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRtemp[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRtemp[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRtemp[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRtemp[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRtemp[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

### Salinity 
qr_5_9_sal <- rq(L_hr_m2 ~ Sal_ppt_Up,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRsal <- summary(qr_5_9_sal, se = "boot", R = 1000)
#sumQRsal

# Graph
qrplot_sal <- ggplot(Filter_only_outRmd_data, aes(Sal_ppt_Up, L_hr_m2)) +
  geom_point(aes(color = Site)) +
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
    labs(x = expression(paste("Salinity (ppt)")),
       y = "",
       title = "B)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRsal[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRsal[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRsal[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRsal[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRsal[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRsal[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

### Turbidity
qr_5_9_turb <- rq(L_hr_m2 ~ Turbidity_NTU_Up,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRturb <- summary(qr_5_9_turb, se = "boot", R = 1000)
#sumQRturb

# Graph
qrplot_turb <- ggplot(Filter_only_outRmd_data, aes(Turbidity_NTU_Up, L_hr_m2)) +
  geom_point(aes(color = Site)) +
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
  labs(x = expression(paste("Turbidity (NTU)")),
       y = "",
       title = "C)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRturb[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRturb[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRturb[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRturb[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRturb[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRturb[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

### TPM
qr_5_9_tpm <- rq(L_hr_m2 ~ Avg_TPM_mg_L,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRtpm <- summary(qr_5_9_tpm , se = "boot", R = 1000)
#sumQRtpm

# Graph
qrplot_tpm <- ggplot(Filter_only_outRmd_data, aes(Avg_TPM_mg_L, L_hr_m2)) +
  geom_point(aes(color = Site)) +
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
  labs(x = expression(paste("Total Particulate Matter (", mu,"g/L)")),
       y = "",
       title = "D)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRtpm[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRtpm[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRtpm[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRtpm[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRtpm[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRtpm[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

### OC 
qr_5_9_oc <- rq(L_hr_m2 ~ Avg_OC_Ratio,
                tau = c(0.5,0.9),
                data = Filter_only_outRmd_data)

# Check results
sumQRoc <- summary(qr_5_9_oc , se = "boot", R = 1000)
#sumQRoc

# Graph
qrplot_oc <- ggplot(Filter_only_outRmd_data, aes(Avg_OC_Ratio, L_hr_m2)) +
  geom_point(aes(color = Site)) +
  geom_hline(yintercept = mean(Filter_only_outRmd_data$L_hr_m2),
             linetype = "dotted", 
             color = "gray50") +
  theme_classic() +
  guides(color=FALSE) +
  labs(x = expression(paste("Organic Content (ratio)")),
       y = "",
       title = "E)") +
  scale_color_manual(values = Site_colors) +
  {if(sumQRoc[[1]]$coefficients[2,4] >= 0.05 | is.na(sumQRoc[[1]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "dashed")} +
  {if(sumQRoc[[1]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.5, color = 'red', linetype = "solid")} +
  {if(sumQRoc[[2]]$coefficients[2,4] >= 0.05 | is.na(sumQRoc[[2]]$coefficients[2,4]))
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "dashed")} +
  {if(sumQRoc[[2]]$coefficients[2,4]< 0.05)
              geom_quantile(quantiles = 0.9, color = 'red', linetype = "solid")}

## Combine graphs into single multiframe figure
## Function to pull out legend
extract_legend<-function(myggplot){
    tmp <- ggplot_gtable(ggplot_build(myggplot))
    leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
    legend <- tmp$grobs[[leg]]
    return(legend)
    }

# Dumbie graph
legend_plot <- ggplot(Filter_only_outRmd_data, aes(Avg_OC_Ratio, L_hr_m2)) +
  geom_point(aes(color = Site)) +
  scale_color_manual(values = Site_colors) +
  theme_classic() 
# extract legend
QR_legend <- extract_legend(legend_plot)

# make multipframe figure
grid.arrange(arrangeGrob(qrplot_temp, qrplot_sal, qrplot_turb, qrplot_tpm, qrplot_oc, QR_legend, 
             nrow = 3,
             left = grid::textGrob(Hab_FR_Label, rot = 90, vjust = 1, 
                                          gp = gpar(fontsize = 12))))

#############################################
# Copy and Pasted from HCR code chunk. Not used up there, not sure if this is redundant to code above. Delete possibly?
############### Quantile Regresssion Analysis Values ################
#### HCR by Site with .5 quantile ###########
# select within each stie
SR_50quant <- Filter_only_outRmd_data %>%
  filter(Site == "San Rafael",
         L_hr_m2 > FR_Site[[1,5]])
#hist(SR_50quant$L_hr_m2, breaks = 20)

MB_50quant <- Filter_only_outRmd_data %>%
  filter(Site == "Morro Bay",
         L_hr_m2 > FR_Site[[2,5]])
#hist(SR_50quant$L_hr_m2, breaks = 20)

NPSM_50quant <- Filter_only_outRmd_data %>%
  filter(Site == "Shellmaker",
         L_hr_m2 > FR_Site[[3,5]])
#hist(NPSM_50quant$L_hr_m2, breaks = 20)

NPD_50quant <- Filter_only_outRmd_data %>%
  filter(Site == "Deanza",
         L_hr_m2 > FR_Site[[4,5]])
#hist(NPD_50quant$L_hr_m2, breaks = 20)

# combine each site's .5 quantile data together
Site_HCR_50quant <- rbind(SR_50quant, MB_50quant, NPSM_50quant, NPD_50quant)

HCR50_sum <- Site_HCR_50quant %>% 
  group_by(Site) %>% 
  summarize(avg_L_hr_m2 = mean(L_hr_m2),
            SD = sd(L_hr_m2),
            n_samples = length(L_hr_m2),
            med = median(L_hr_m2),
            var = var(L_hr_m2))
# 443514/6689 --> largest residual / smallest residual - greater than 5 not good for ANOVA

HCR_50quant_box <- ggplot((Site_HCR_50quant), aes(Site, L_hr_m2)) +
  geom_point(aes(fill = Site), shape = 21, show.legend = FALSE) +
  scale_fill_manual(values = Site_colors) +
  theme_classic() +
  labs(x = 'Site',
       y = Hab_FR_Label)

#plot(HCR_50quant_box)

####### Kruskal-Wallis test - tidy converts to clean dataframe
KW_HCR50_site <- broom::tidy(kruskal.test(L_hr_m2 ~ Site, data = Site_HCR_50quant))


####### one-way ANOVA -- HCR by Site
HCR50_site_aov <- aov(L_hr_m2 ~ Site, data = Site_HCR_50quant)
#summary(HCR_site_aov)
HCR50_aov_values <- broom::tidy(HCR50_site_aov)

########### Power analysis - sensitivity power analysis 
# one-way ANOVA effect size (n^2). n^2 = treatmentSumSquares / TotalSumSquares (TreatSS + ResidualSS) 
d_HCR50_site <- { HCR50_aov_values[[1,3]] / (HCR50_aov_values[[1,3]] + HCR50_aov_values[[2,3]]) }

# power analysis for "balanced one-way analysis of variance tests"
# For now Ted says to use the mean of the different sample sizes
HCR50_site_power <- pwr.anova.test(f = d_HCR50_site, 
                                 k = 4,
                                 n = mean(HCR50_sum$n_samples), 
                                 sig.level = 0.05,
                                 power = NULL) # solve for this
```