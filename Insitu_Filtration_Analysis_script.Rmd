---
title: "Oyster Insitu Filtration"
output: 
  flexdashboard::flex_dashboard:
    theme: yeti
    orientation: rows
editor_options: 
  chunk_output_type: console
---


```{r setup}

## Import the libraries and functions
library(flexdashboard) # knit output format
library(dplyr) # data manupulation 
library(tidyr) # data rearranging
library(data.table)
library(magrittr)
library(lubridate) # dealing with dates
library(ggplot2) # graphing
library(ggthemes) # theme_gdocs() in graphs
library(stringr) # dealing with strings
library(hms) # dealing with time & date
library(wesanderson) # color palatte
library(ggpubr) 
library(cowplot) # arragne multiple plots or objects in single plot
library(broom) # tidy(t.test()) converts t-test output from console text to 1 x 8 tibble 
library(olsrr)

######################################################################
## settings: 
#1) data_directory: Keep only the input csv time series data in this directory
#2) velocity_data_directory : Additional files required to pair filtration time stamps by Velocity to be kept here
#3) manual_correction_directory: Put the manual correction files here. Can be the same as velocity_data_directory
#4) output_directory: This is the directory where the output will be created 
#5) code_directory : The directory where this code and the functions file are stored
######################################################################

data_directory = "./Data/sonde_time_series"
velocity_data_directory = "./Data"
manual_correction_directory = "./manual_corrections"
output_directory = "./output"
# V_corr_Filtration_dir = "./output/4_Filtration_Calculations" # Velocity Corrected Filtration 

######################################################################
# Source/import required files 
######################################################################
# Source the functions
source( "functions.R")

# Create the required output directories
createOutputDirectories()

# Import the files related to water velocity measurements 
water_vel_df = fread(file.path(velocity_data_directory, "Insitu_Filter_velocity_all_data.csv"))
  
water_vel_other_var_df = fread(file.path(velocity_data_directory, "Insitu_Filter_sbs_FR_variables_all_data.csv"))

water_vel_summary = calculateTravelTimeBySiteAndDate(water_vel_df, water_vel_other_var_df)
fwrite(water_vel_summary, file.path(water_velocity_summary_directory, "Water_V_summary.csv"))

# Import the manual corrections file
manual_correction_df = fread(file.path(manual_correction_directory, "Manual_Corrections.csv"))

manual_correction_df %<>%
  dplyr::mutate(Correction_Start_Time = as.hms(Correction_Start_Time),
                Correction_End_Time   = as.hms(Correction_End_Time))
```


Chl - Raw Data
=====================================

### 

```{r Std_names_plot_raw_chl, fig.height=5, fig.width=13.5, cache=TRUE}

######################################################################
##Steps 1 to 4) read the files to standardize names and plot time series for sondes
######################################################################
all_files = list.files(data_directory, pattern = ".csv")

# Loop through files to standardize names and plot graph
for(file in 1 : length(all_files))
{
  file_name = all_files[file]
  
  # 1) Import the file
  one_file = fread(file.path(data_directory, file_name))
  
  #2 & 3) standardize name of columns and values
  one_file = standardizeNamesAndColumns(one_file)
 
  # 4) Plot the graph 
  plot(createTimeSeriesPlot(one_file, file_name, orig_graphs_directory, "Bef_Corr"))
  
  ## save the modified file 
  fwrite(one_file, file.path(output_dir_file_cleaning, file_name))
  
}
```

Chl - Spikes Removed
=====================================

### 

```{r Manual_Sbs_Corr_Plot_Chl_FR_Clac, fig.height=5, fig.width=13.5, cache=TRUE}
######################################################################
##Step 5 to 8)  Apply manual, Sbs corrections and calculate filtration
######################################################################
# empty data frame to store sbs correction summary  
sbs_correction_summary = data.frame()
filtration_summary_table = data.frame()

#loop through files and apply manual corrections
for(file in 1 : length(all_files))
{

######################################################################
##Step 5)  Apply manual corrections
######################################################################
    
  file_name = all_files[file]
    
  # Import the file
  one_file = fread(file.path(output_dir_file_cleaning, file_name))
    
  ## apply manual corrections
  one_file = applyManualCorrections(one_file, file_name, manual_correction_df,
                                    output_dir_manual_corrections)
    
  # plot the time series agains
  plot(createTimeSeriesPlot(one_file, file_name, corrected_graphs_directory, "After_Corr"))
    
  fwrite(one_file, file.path(output_dir_manual_corrections, file_name))
  
######################################################################
##Step 6) Apply sbs corrections 
######################################################################
  
  # check if sbs correction is required and record the result
  sbs_summary_one_file = summarizeSbsCorrectionValues(one_file, file_name)
  sbs_correction_summary = rbind(sbs_summary_one_file, sbs_correction_summary)
  
  # apply the correction if required
  one_file =  applySbsCorrections(one_file, sbs_summary_one_file)
  
  # save the corrected file
  fwrite(one_file, file.path(sbs_correction_output_directory, file_name))
  
######################################################################
##Step 7 and 8) Water Velocity measurement and Filtration calculation
######################################################################
  one_file = adjustDownSondeTimeStamp(water_vel_summary, one_file)
  one_file = calculateFiltrationForPairedData(one_file, water_vel_summary)
  
  if(nrow(one_file) > 0)
  {  
    one_filtration_summary = createFiltrationSummary(one_file, file_name, water_vel_summary)
    filtration_summary_table = rbind(filtration_summary_table, one_filtration_summary)
  }
  # save the filtration calculations
  fwrite(one_file, file.path(filtration_calc_output_directory, file_name))

}

# Save the correction summary
fwrite(sbs_correction_summary, file.path(sbs_summary_output_directory,
                                        "sbs_correction_summary.csv"))

fwrite(filtration_summary_table, file.path(filtration_summary_output_directory,
                                        "filtration_summary.csv"))
  

```

SBS - Correction Calc
=====================================

###
```{r Sbs_Correction_Uncertainty, fig.height=5, fig.width=4, cache=TRUE}
######################################################################
# Create SBS Stats - Uncertainty Table & Graph SBS Density Plots
######################################################################
# empty data frame to store sbs stats summary  
sbs_stats_summary = data.frame()

for(file in 1 : length(all_files))
{
  
  file_name = all_files[file]
    
  # Import the file
  one_file = fread(file.path(output_dir_manual_corrections, file_name))
  
  # generate data for Sbs density graphs
  stats_for_graph = calculateErrorStats(one_file, file_name, manual_correction_df, error_directory)
  Sbs_Graph_data = calculateSbsGraphData(one_file)  
  # plot Sbs density graphs
  plot(createSbsDensityPlot(Sbs_Graph_data, stats_for_graph, file_name))
  
  if(nrow(one_file) > 0)
  {  
    ## calculate Statistics on Sbs data
    one_file_stats = calculateErrorStats(one_file, file_name, manual_correction_df,
                                    error_directory)
    sbs_stats_summary = rbind(sbs_stats_summary, one_file_stats) 
  }
}

# Save the SBS Statistics summary
fwrite(sbs_stats_summary, file.path(error_directory,
                                        "Sbs_stats_error_summary.csv"))
```

Chl Drawdown - Velocity paired
=====================================

###

```{r Plot_V_paired_Chl_diff, fig.height=5, fig.width=10, cache=TRUE}
######################################################################
## Graph Filtration Time Series After Veloctiy Time Adjustment (AM trying to copy for loop)
######################################################################


# directs where files are
all_files_Filtration = list.files(filtration_calc_output_directory, pattern = ".csv")

# Loop like Task 5 with different plot function
for(file in 1 : length(all_files_Filtration))
{ 
    
  file_name = all_files_Filtration[file]
    
  # Import the file
  one_file = fread(file.path(filtration_calc_output_directory, file_name))
    
  # plot the time series agains
  if(nrow(one_file) > 0){
  plot(createChlDiffPlot(one_file, file_name, pairedChlDiff_graphs_directory,
                         "Chl_diff_vel_paired"))
  }
}

```


```{r TPM_POM_PIM, cache=TRUE}

#######################################################################
# Create directories for TPM 
#######################################################################

TPM_directory = "./Data"
#TPM_output_directory creation is in functions.R

#######################################################################
# Read in data for TPM 
#######################################################################

TPM_data = fread(file.path(TPM_directory, "Insitu_Filter_POM_PIM_Data.csv"))

######################################################################
# Summary Table & Average Table per field day
######################################################################

TPM_summary_table <- TPM_data %>% 
  mutate(TPM = ((dry_weight_corrected_g - filter_weight_corrected_g) * 1000 / sample_volume_ml),
         PIM = TPM - ((ash_weight_corrected_g - filter_weight_corrected_g) * 1000 / sample_volume_ml),
         POM = TPM - PIM,
         Organic_Content_Ratio = POM / TPM) %>% 
  as_tibble()

Avg_TPM_summary_table <- TPM_summary_table %>% 
  group_by(Date, Site, sample_type) %>% 
  summarize(Avg_TPM = round(mean(TPM), 4),
            Avg_PIM = round(mean(PIM), 4),
            Avg_POM = round(mean(POM), 4),
            Avg_OC_Ratio = round(mean(Organic_Content_Ratio), 4)) 

#####################################################################
# Export Summary Tables to project Output folder
######################################################################

fwrite(TPM_summary_table, file.path(TPM_output_directory, "TPM_summary_table.csv"))
fwrite(Avg_TPM_summary_table, file.path(TPM_output_directory, "Avg_TPM_summary_table.csv"))

```

```{r Total_bivlave_density, cache=TRUE}
#####################################################################
# Total Bivalve Density - Data Directories
#######################################################################

SR_Density_directory = "./Data/bivalve_density_community"
NP_Density_directory = "./Data/bivalve_density_community"
# Bivalve_output_directory created in functions.R

#####################################################################
# Read in data
#######################################################################

SR_density_data = fread(file.path(SR_Density_directory, "Insitu_Filter_SR_oyster_density.csv"))
NP_density_data = fread(file.path(NP_Density_directory, "Insitu_filter_NP_may18_survey_counts.csv"))

# San Rafael bivalve density based on data from Chela Zabin (November 2017 survey)
#SR_nov17_olympia_m2 <- mean(SR_density_data$density_m2)      

# Morro Bay total bivalve density - data from MBOC staff 
MB_gigas_m2 = 600

# Newport Deanza total bivalve Density - Data from Zacherl lab surveys 
NP_excavation_quad_area = 0.0625

########## May not need these calculations
NPD_may18_bivalve_den <- NP_density_data %>% 
  filter(Site == 'Deanza') %>% 
  select(1:10, 12) %>% 
  gather('O. lurida', 'C. gigas', 'Mytilus', 'Musculista', 'Geukensia', 'Adula', 'Speckled_scallop',
         key = 'Species',
         value = 'n_individuals') %>% 
  group_by(Quadrat) %>% 
  summarise(n_bivalves = sum(n_individuals),
            quad_den = n_bivalves / NP_excavation_quad_area)
  
NPD_may18_avg_m2 <- mean(NPD_may18_bivalve_den$quad_den) # Total Average bivalve density

# Newport Shellmaker total bivalve density
########## May not need these calculations
NPSM_may18_bivalve_den <- NP_density_data %>% 
  filter(Site == 'Shellmaker') %>% 
  select(1:10, 12) %>% 
  gather('O. lurida', 'C. gigas', 'Mytilus', 'Musculista', 'Geukensia', 'Adula', 'Speckled_scallop',
         key = 'Species',
         value = 'n_individuals') %>% 
  group_by(Quadrat) %>% 
  summarise(n_bivalves = sum(n_individuals),
            quad_den = n_bivalves / NP_excavation_quad_area)

NPSM_may18_avg_m2 <- mean(NPSM_may18_bivalve_den$quad_den) # Total Average bivalve density
  
#####################################################################
# Densities by Bivalve Species
######################################################################
# dataframe density by species - Deanza

NPD_may18_species_den <- NP_density_data %>% 
  filter(Site == 'Deanza') %>% 
 # select need columns & replace species names with full scientific names 
  rename(Site = 1,
         Treatment = 2, 
         Quadrat = 3, 
         Date = 4, 
         Ostrea_lurida = 5, 
         Crassostrea_gigas = 6,
         Mytilus_galloprovincialis = 7, 
         Musculista_senhousia = 8,
         Geukensia_demissa = 9, 
         Adula_diegensis = 10,
         Argopecten_ventricosa = 12) %>% 
  gather('Ostrea_lurida', 'Crassostrea_gigas', "Mytilus_galloprovincialis", "Musculista_senhousia",
         "Geukensia_demissa", "Adula_diegensis", "Argopecten_ventricosa",
         key = 'Species',
         value = 'n_individuals') %>% 
  mutate(Date = mdy(Date)) %>% 
  group_by(Site, Date, Species) %>% 
  summarise(species_den_m2 = (sum(n_individuals) / n()) / NP_excavation_quad_area) %>% 
  # remove species with 0 individuals per m^2
  filter(!(species_den_m2 <= 0))
  
  
# dataframe density by species - Shellmaker
NPSM_may18_species_den <- NP_density_data %>% 
  filter(Site == 'Shellmaker') %>% 
 # select need columns & replace species names with full scientific names 
  rename(Site = 1,
         Treatment = 2, 
         Quadrat = 3, 
         Date = 4, 
         Ostrea_lurida = 5, 
         Crassostrea_gigas = 6,
         Mytilus_galloprovincialis = 7, 
         Musculista_senhousia = 8,
         Geukensia_demissa = 9, 
         Adula_diegensis = 10,
         Argopecten_ventricosa = 12) %>% 
  gather('Ostrea_lurida', 'Crassostrea_gigas', "Mytilus_galloprovincialis", "Musculista_senhousia",
         "Geukensia_demissa", "Adula_diegensis", "Argopecten_ventricosa",
         key = 'Species',
         value = 'n_individuals') %>% 
  mutate(Date = mdy(Date)) %>% 
  group_by(Site, Date, Species) %>% 
  summarise(species_den_m2 = (sum(n_individuals) / n()) / NP_excavation_quad_area) %>% 
  # remove species with 0 individuals per m^2
  filter(!(species_den_m2 <= 0))

# dataframe density by species - San Rafael
SR_nov17_species_den <- SR_density_data %>% 
  mutate(Species = "Ostrea_lurida",
         Date = mdy("11/1/17")) %>% # survey date from Chela Zabin
  group_by(Site, Date, Species) %>% 
  summarise(species_den_m2 = mean(density_m2))
  
# dataframe density by species - numbers from Morro Bay Oyster Company
MB_species_den <- tibble(Site = "Morro Bay",
                         Date = mdy("11/21/17"), # Date info was recorded in field notebook 
                         Species = "Crassostrea_gigas", 
                         species_den_m2 = 600)

# combine species density dataframes - All sites
Species_density_summary <- bind_rows(NPD_may18_species_den,
                                     NPSM_may18_species_den,
                                     SR_nov17_species_den,
                                     MB_species_den, .id = NULL)

fwrite(Species_density_summary, file.path(Bivalve_output_directory, "Bivalve_Density_summary.csv"))

```

```{r Create_Master_Analysis_Table, cache=TRUE}
##############################################################
# Create Master Analysis Table
##############################################################
Master_analysis_table <- filtration_summary_table %>% 
  left_join(Avg_TPM_summary_table, by = c("Date", "Site")) %>% 
  # Exclude downstream average WQ values and unneeded single tail t-test results
  select(-c("estimate", "conf.high", "sample_type","Temp_C_Down", "SpCond_mS_cm_Down",
            "Cond_mS_cm_Down", "TDS_g_L_Down", "Sal_ppt_Down", "Turbidity_NTU_Down")) %>% 
  # Add in site simple variables for regression
  mutate(SR = ifelse(grepl("San Rafael", filtration_summary_table$Site), T, F),
         NPD = ifelse(grepl("Deanza", filtration_summary_table$Site), T, F),
         NPSM = ifelse(grepl("Shellmaker", filtration_summary_table$Site), T, F),
         MB = ifelse(grepl("Morro Bay", filtration_summary_table$Site), T, F)) 

# fwrite(Master_analysis_table, file.path('./Data', "Master_Filtration_analysis_table.csv"))
fwrite(Master_analysis_table, file.path(filtration_summary_output_directory,
                                        "Master_Filtration_analysis_table.csv"))
```

```{r Species_Density_Graphs, echo=FALSE, cache=TRUE}
# Graph Density by Species, exclude absent species

############## Newport Deanza ###################
(Graph_NPD_species_den <-
   ggplot(NPD_may18_species_den[which(NPD_may18_species_den$species_den_m2>0),], 
                               aes(Species, species_den_m2, fill = Species)) + 
  geom_col() + # columns
  theme_classic() + 
  labs (x = 'Bivlave Species', 
        y = 'Density (individuals / m^2)', 
        title = 'Newport Deanza Bivalve Density',
        subtitle = 'May 2018 Survey') +
  guides(fill = FALSE) + # removes color legend
  geom_text(aes(label = species_den_m2), vjust = -0.5)) # able to round values? 
                                                                      
############## Newport Shellmaker ##############
(Graph_NPSM_species_den <-   ggplot(NPSM_may18_species_den[which(NPD_may18_species_den$species_den_m2>0),], 
                                 aes(Species, species_den_m2, fill = Species)) + 
  geom_col() + # columns
  theme_classic() + 
  labs (x = 'Bivlave Species', 
        y = 'Density (individuals / m^2)', 
        title = 'Newport Shellmaker Bivalve Density',
        subtitle = 'May 2018 Survey') +
  guides(fill = FALSE) + # removes color legend
  geom_text(aes(label = species_den_m2), vjust = -0.5)) # able to round values? 


# Save graph to "./0_Graph_Output/Graphs_Bivalve_Density"
# Not needed Whit Rmd Knit yet
######### May need to reformat .pdf  ###########
#ggsave(file.path(Bivlave_density_graph_directory, "NPD_may18_bivalve_species_density.pdf"), Graph_NPD_species_den )
#ggsave(file.path(Bivlave_density_graph_directory, "NPSM_may18_bivalve_species_density.pdf"), Graph_NPSM_species_den )

##### FIgure ####### -->> biomass comparison among species
```