---
title: "Oyster Insitu Filtration"
output: 
  flexdashboard::flex_dashboard:
    theme: yeti
    orientation: rows
editor_options: 
  chunk_output_type: console
---


```{r setup}

## Import the libraries and functions
library(flexdashboard) # knit output format
library(dplyr) # data manupulation 
library(tidyr) # data rearranging
library(data.table)
library(magrittr)
library(lubridate) # dealing with dates
library(ggplot2) # graphing
library(ggthemes) # theme_gdocs() in graphs
library(stringr) # dealing with strings
library(hms) # dealing with time & date
library(wesanderson) # color palatte
library(ggpubr) 
library(cowplot) # arragne multiple plots or objects in single plot
library(broom) # tidy(t.test()) converts t-test output from console text to 1 x 8 tibble 
library(olsrr)

######################################################################
## settings: 
#1) data_directory: Keep only the input csv time series data in this directory
#2) velocity_data_directory : Additional files required to pair filtration time stamps by Velocity to be kept here
#3) manual_correction_directory: Put the manual correction files here. Can be the same as velocity_data_directory
#4) output_directory: This is the directory where the output will be created 
#5) code_directory : The directory where this code and the functions file are stored
######################################################################

data_directory = "./Data/sonde_time_series"
velocity_data_directory = "./Data"
manual_correction_directory = "./manual_corrections"
output_directory = "./output"
# V_corr_Filtration_dir = "./output/4_Filtration_Calculations" # Velocity Corrected Filtration 

######################################################################
# Source/import required files 
######################################################################
# Source the functions
source( "functions.R")

# Create the required output directories
createOutputDirectories()

# Import the files related to water velocity measurements 
water_vel_df = fread(file.path(velocity_data_directory, "Insitu_Filter_velocity_all_data.csv"))
  
water_vel_other_var_df = fread(file.path(velocity_data_directory, "Insitu_Filter_sbs_FR_variables_all_data.csv"))

water_vel_summary = calculateTravelTimeBySiteAndDate(water_vel_df, water_vel_other_var_df)
fwrite(water_vel_summary, file.path(water_velocity_summary_directory, "Water_V_summary.csv"))

# Import the manual corrections file
manual_correction_df = fread(file.path(manual_correction_directory, "Manual_Corrections.csv"))

manual_correction_df %<>%
  dplyr::mutate(Correction_Start_Time = as.hms(Correction_Start_Time),
                Correction_End_Time   = as.hms(Correction_End_Time))
```


Chl - Raw Data
=====================================

### 

```{r Std_names_plot_raw_chl, fig.height=5, fig.width=13.5, cache=TRUE}

######################################################################
##Steps 1 to 4) read the files to standardize names and plot time series for sondes
######################################################################
all_files = list.files(data_directory, pattern = ".csv")

# Loop through files to standardize names and plot graph
for(file in 1 : length(all_files))
{
  file_name = all_files[file]
  
  # 1) Import the file
  one_file = fread(file.path(data_directory, file_name))
  
  #2 & 3) standardize name of columns and values
  one_file = standardizeNamesAndColumns(one_file)
 
  # 4) Plot the graph 
  plot(createTimeSeriesPlot(one_file, file_name, orig_graphs_directory, "Bef_Corr"))
  
  ## save the modified file 
  fwrite(one_file, file.path(output_dir_file_cleaning, file_name))
  
}
```

Chl - Spikes Removed
=====================================

### 

```{r Manual_Sbs_Corr_Plot_Chl_FR_Clac, fig.height=5, fig.width=13.5, cache=TRUE}
######################################################################
##Step 5 to 8)  Apply manual, Sbs corrections and calculate filtration
######################################################################
# empty data frame to store sbs correction summary  
sbs_correction_summary = data.frame()
filtration_summary_table = data.frame()

#loop through files and apply manual corrections
for(file in 1 : length(all_files))
{

######################################################################
##Step 5)  Apply manual corrections
######################################################################
    
  file_name = all_files[file]
    
  # Import the file
  one_file = fread(file.path(output_dir_file_cleaning, file_name))
    
  ## apply manual corrections
  one_file = applyManualCorrections(one_file, file_name, manual_correction_df,
                                    output_dir_manual_corrections)
    
  # plot the time series agains
  plot(createTimeSeriesPlot(one_file, file_name, corrected_graphs_directory, "After_Corr"))
    
  fwrite(one_file, file.path(output_dir_manual_corrections, file_name))
  
######################################################################
##Step 6) Apply sbs corrections 
######################################################################
  
  # check if sbs correction is required and record the result
  sbs_summary_one_file = summarizeSbsCorrectionValues(one_file, file_name)
  sbs_correction_summary = rbind(sbs_summary_one_file, sbs_correction_summary)
  
  # apply the correction if required
  one_file =  applySbsCorrections(one_file, sbs_summary_one_file)
  
  # save the corrected file
  fwrite(one_file, file.path(sbs_correction_output_directory, file_name))
  
######################################################################
##Step 7 and 8) Water Velocity measurement and Filtration calculation
######################################################################
  one_file = adjustDownSondeTimeStamp(water_vel_summary, one_file)
  one_file = calculateFiltrationForPairedData(one_file, water_vel_summary)
  
  if(nrow(one_file) > 0)
  {  
    one_filtration_summary = createFiltrationSummary(one_file, file_name, water_vel_summary)
    filtration_summary_table = rbind(filtration_summary_table, one_filtration_summary)
  }
  # save the filtration calculations
  fwrite(one_file, file.path(filtration_calc_output_directory, file_name))

}

# Save the correction summary
fwrite(sbs_correction_summary, file.path(sbs_summary_output_directory,
                                        "sbs_correction_summary.csv"))

fwrite(filtration_summary_table, file.path(filtration_summary_output_directory,
                                        "filtration_summary.csv"))
  

```

SBS - Correction Calc
=====================================

###
```{r Sbs_Correction_Uncertainty, fig.height=5, fig.width=4, cache=TRUE}
######################################################################
# Create SBS Stats - Uncertainty Table & Graph SBS Density Plots
######################################################################
# empty data frame to store sbs stats summary  
sbs_stats_summary = data.frame()

for(file in 1 : length(all_files))
{
  
  file_name = all_files[file]
    
  # Import the file
  one_file = fread(file.path(output_dir_manual_corrections, file_name))
  
  # generate data for Sbs density graphs
  stats_for_graph = calculateErrorStats(one_file, file_name, manual_correction_df, error_directory)
  Sbs_Graph_data = calculateSbsGraphData(one_file)  
  # plot Sbs density graphs
  plot(createSbsDensityPlot(Sbs_Graph_data, stats_for_graph, file_name))
  
  if(nrow(one_file) > 0)
  {  
    ## calculate Statistics on Sbs data
    one_file_stats = calculateErrorStats(one_file, file_name, manual_correction_df,
                                    error_directory)
    sbs_stats_summary = rbind(sbs_stats_summary, one_file_stats) 
  }
}

# Save the SBS Statistics summary
fwrite(sbs_stats_summary, file.path(error_directory,
                                        "Sbs_stats_error_summary.csv"))
```

Chl Drawdown - Velocity paired
=====================================

###

```{r Plot_V_paired_Chl_diff, fig.height=5, fig.width=10, cache=TRUE}
######################################################################
## Graph Filtration Time Series After Veloctiy Time Adjustment (AM trying to copy for loop)
######################################################################


# directs where files are
all_files_Filtration = list.files(filtration_calc_output_directory, pattern = ".csv")

# Loop like Task 5 with different plot function
for(file in 1 : length(all_files_Filtration))
{ 
    
  file_name = all_files_Filtration[file]
    
  # Import the file
  one_file = fread(file.path(filtration_calc_output_directory, file_name))
    
  # plot the time series agains
  if(nrow(one_file) > 0){
  plot(createChlDiffPlot(one_file, file_name, pairedChlDiff_graphs_directory,
                         "Chl_diff_vel_paired"))
  }
}

```

```{r TPM_directories, cache = T}
#######################################################################
# Create directories for TPM 
#######################################################################

TPM_directory = "./Data"
#TPM_output_directory creation is in functions.R

#######################################################################
# Read in data for TPM 
#######################################################################

TPM_data = fread(file.path(TPM_directory, "Insitu_Filter_POM_PIM_Data.csv"))

######################################################################
# Summary Table & Average Table per field day
######################################################################

TPM_summary_table <- TPM_data %>% 
  mutate(TPM_mg_L = (((dry_weight_corrected_g - filter_weight_corrected_g)*1000) / (sample_volume_ml/1000)), # grams to mg
         PIM_mg_L = (((ash_weight_corrected_g - filter_weight_corrected_g)*1000) / (sample_volume_ml/1000)),
         POM_mg_L = TPM_mg_L - PIM_mg_L,
         Organic_Content_Ratio = POM_mg_L / TPM_mg_L) %>% 
  as_tibble()

Avg_TPM_summary_table <- TPM_summary_table %>% 
  group_by(Date, Site, sample_type) %>% 
  summarize(Avg_TPM_mg_L = round(mean(TPM_mg_L), 4),
            Avg_PIM_mg_L = round(mean(PIM_mg_L), 4),
            Avg_POM_mg_L = round(mean(POM_mg_L), 4),
            Avg_OC_Ratio = round(mean(Organic_Content_Ratio), 4)) 

#####################################################################
# Export Summary Tables to project Output folder
######################################################################

fwrite(TPM_summary_table, file.path(TPM_output_directory, "TPM_summary_table.csv"))
fwrite(Avg_TPM_summary_table, file.path(TPM_output_directory, "Avg_TPM_summary_table.csv"))

```

```{r Total_bivlave_density, cache=TRUE}
#####################################################################
# Total Bivalve Density - Data Directories
#######################################################################

SR_Density_directory = "./Data/bivalve_density_community"
NP_Density_directory = "./Data/bivalve_density_community"
# Bivalve_output_directory created in functions.R

#####################################################################
# Read in data
#######################################################################

SR_density_data = fread(file.path(SR_Density_directory, "Insitu_Filter_SR_oyster_density.csv"))
NP_density_data = fread(file.path(NP_Density_directory, "Insitu_filter_NP_may18_survey_counts.csv"))

# San Rafael bivalve density based on data from Chela Zabin (November 2017 survey)
#SR_nov17_olympia_m2 <- mean(SR_density_data$density_m2)      

# Morro Bay total bivalve density - data from MBOC staff 
MB_gigas_m2 = 600

# Newport Deanza total bivalve Density - Data from Zacherl lab surveys 
NP_excavation_quad_area = 0.0625
  
#####################################################################
# Densities by Bivalve Species
######################################################################
# dataframe density by species - Deanza

NPD_may18_species_den <- NP_density_data %>% 
  filter(Site == 'Deanza') %>% 
 # select need columns & replace species names with full scientific names 
  rename(Site = 1,
         Treatment = 2, 
         Quadrat = 3, 
         Date = 4, 
         Ostrea_lurida = 5, 
         Crassostrea_gigas = 6,
         Mytilus_galloprovincialis = 7, 
         Musculista_senhousia = 8,
         Geukensia_demissa = 9, 
         Adula_diegensis = 10,
         Argopecten_ventricosa = 12) %>% 
  gather('Ostrea_lurida', 'Crassostrea_gigas', "Mytilus_galloprovincialis", "Musculista_senhousia",
         "Geukensia_demissa", "Adula_diegensis", "Argopecten_ventricosa",
         key = 'Species',
         value = 'n_individuals') %>% 
  mutate(Date = mdy(Date)) %>% 
  group_by(Site, Date, Species) %>% 
  summarise(species_den_m2 = (sum(n_individuals) / n()) / NP_excavation_quad_area) %>% 
  # remove species with 0 individuals per m^2
  filter(!(species_den_m2 <= 0))
  
  
# dataframe density by species - Shellmaker
NPSM_may18_species_den <- NP_density_data %>% 
  filter(Site == 'Shellmaker') %>% 
 # select need columns & replace species names with full scientific names 
  rename(Site = 1,
         Treatment = 2, 
         Quadrat = 3, 
         Date = 4, 
         Ostrea_lurida = 5, 
         Crassostrea_gigas = 6,
         Mytilus_galloprovincialis = 7, 
         Musculista_senhousia = 8,
         Geukensia_demissa = 9, 
         Adula_diegensis = 10,
         Argopecten_ventricosa = 12) %>% 
  gather('Ostrea_lurida', 'Crassostrea_gigas', "Mytilus_galloprovincialis", "Musculista_senhousia",
         "Geukensia_demissa", "Adula_diegensis", "Argopecten_ventricosa",
         key = 'Species',
         value = 'n_individuals') %>% 
  mutate(Date = mdy(Date)) %>% 
  group_by(Site, Date, Species) %>% 
  summarise(species_den_m2 = (sum(n_individuals) / n()) / NP_excavation_quad_area) %>% 
  # remove species with 0 individuals per m^2
  filter(!(species_den_m2 <= 0))

# dataframe density by Treatment (Oyster & Eelgrass_oyster) - San Rafael
SR_nov17_den_summary <- SR_density_data %>% 
  mutate(Species = "Ostrea_lurida",
         Date = mdy("11/1/17")) %>% # survey date from Chela Zabin
  group_by(Site, Treatment, Date, Species) %>% 
  summarise(species_den_m2 = mean(density_m2))

# select only Oyster treatment
SR_nov17_species_den <- SR_nov17_den_summary %>% 
  filter(Treatment %in% c("Oyster")) %>% # only use Oyster treatment
  ungroup() %>% # ungroup
  dplyr::select(Site, Date, Species, species_den_m2) # select all but treatment column 
  
# dataframe density by species - numbers from Morro Bay Oyster Company
MB_species_den <- tibble(Site = "Morro Bay",
                         Date = mdy("11/21/17"), # Date info was recorded in field notebook 
                         Species = "Crassostrea_gigas", 
                         species_den_m2 = 600)

# combine species density dataframes - All sites
Bivalve_summary <- bind_rows(NPD_may18_species_den,
                                     NPSM_may18_species_den,
                                     SR_nov17_species_den,
                                     MB_species_den, .id = NULL)

fwrite(Bivalve_summary, file.path(Bivalve_output_directory, "Bivalve_Site_summary.csv"))

Species_density_summary <- Bivalve_summary %>% 
  pivot_wider(names_from = Species,
              values_from = species_den_m2) %>% 
  dplyr::select(Site, Date,
         "Adula_diegensis_m2" = Adula_diegensis,
         "Musculista_senhousia_m2" = Musculista_senhousia,
         "Mytilus_galloprovincialis_m2" = Mytilus_galloprovincialis,
         "Ostrea_lurida_m2" = Ostrea_lurida,
         "Argopecten_ventricosa_m2" = Argopecten_ventricosa,
         "Geukensia_demissa_m2" = Geukensia_demissa,
         "Crassostrea_gigas_m2" = Crassostrea_gigas) %>% 
  # calculate total bivalve density
  mutate(Bivalve_den_m2 = sum(Adula_diegensis_m2, Musculista_senhousia_m2, Mytilus_galloprovincialis_m2,
           Ostrea_lurida_m2, Argopecten_ventricosa_m2, Geukensia_demissa_m2, Crassostrea_gigas_m2, na.rm = T))

fwrite(Species_density_summary, file.path(Bivalve_output_directory, "Bivalve_Density_summary.csv"))

```

```{r Create_Master_Analysis_Table, cache=TRUE}
##############################################################
# Create Master Analysis Table
##############################################################
filtration_summary_table <- fread("output/4_Filtration_Calculations/Filtration_Summary/filtration_summary.csv")

Master_analysis_table <- filtration_summary_table %>% 
  left_join(Avg_TPM_summary_table, by = c("Date", "Site")) %>% 
  # Exclude downstream average WQ values and unneeded single tail t-test results
  dplyr::select(-c("Temp_C_Down", "SpCond_mS_cm_Down",
            "Cond_mS_cm_Down", "TDS_g_L_Down", "Sal_ppt_Down", "Turbidity_NTU_Down"))#  %>% 
  # Add in site simple variables for regression - removed b/c not using Multiple Linear Regression
  # mutate(SR = ifelse(grepl("San Rafael", filtration_summary_table$Site), T, F),
   #      NPD = ifelse(grepl("Deanza", filtration_summary_table$Site), T, F),
    #     NPSM = ifelse(grepl("Shellmaker", filtration_summary_table$Site), T, F),
     #    MB = ifelse(grepl("Morro Bay", filtration_summary_table$Site), T, F),
      #   Filter_trial = ifelse(grepl("Filtration", filtration_summary_table$Experiment), T, F)) %>% 
  left_join(Species_density_summary, by = c("Site")) %>% 
  dplyr::select(-c("Date.y")) %>% 
  rename(Date = Date.x)
  

fwrite(Master_analysis_table, file.path(filtration_summary_output_directory,
                                        "Master_Filtration_analysis_table.csv"))
```
